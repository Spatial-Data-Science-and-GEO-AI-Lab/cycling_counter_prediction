{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing needed libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ee'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m \n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mee\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrasterio\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ee'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import ee\n",
    "import time\n",
    "import rasterio\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import geopandas as gpd \n",
    "import shapely.geometry\n",
    "from rasterstats import zonal_stats\n",
    "from pylandtemp import single_window\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting initial varibles\n",
    "\n",
    "Adjust the variables below in order to fine tune the calculations done throughout the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the directory for download and directory of counter file\n",
    "os.chdir('/Users/winke/Documents/University/Thesis/Extracts')\n",
    "\n",
    "#Set the crs projection to work in\n",
    "crs = 3857\n",
    "\n",
    "#Read the counter data into the program\n",
    "df = pd.read_csv(\"weekly_counts.csv\")\n",
    "\n",
    "#Select cities and create a list that can be iterated through\n",
    "citynames_l = df['city'].drop_duplicates().tolist()\n",
    "\n",
    "#Checking working directory and creating path\n",
    "directory = os.getcwd() #use working directory or desired location\n",
    "connect = \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amsterdam', 'Rotterdam', 'New York', 'Seattle', 'Utrecht', 'Norfolk, Virginia', 'Arlington County', 'Austin', 'Minneapolis', 'Almere', 'Arnhem', 'London', 'Northampton', 'Leeds', 'Birmingham', 'Nijmegen', 'Apeldoorn', 'York', 'Duluth', 'Maastricht', 'Boulder', 'Portland', 'Cambridge', 'Denver', 'Philadelphia', 'Houston', 'Dallas', 'Glasgow City', 'Edinburgh', 'Aberdeen', 'Wageningen', 'Gouda', 'San Diego', 'Venlo', 'Amersfoort', 'Tampa', 'Gainesville', 'Tallahassee', 'Fort Lauderdale', 'Miami', 'The Hague', 'Leiden', 'Zeist', 'Dordrecht', 'Ede', 'Raleigh', 'Cary', 'Durham', 'Greensboro', 'Charlotte']\n"
     ]
    }
   ],
   "source": [
    "print(citynames_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For exporting particular cities, list them here\n",
    "citynames_l = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating city folders and extracting city bounds\n",
    "\n",
    "In order to be able to extract geographic variables for each of the cities included in the analysis, the bounds of the city will have to be determined. The bounds are extracted from the OSMnx API and converted into a rectangular shape, ecompassing the total extent of the city, as defined by OSMnx. Additionally, a small buffer is added to ensure that counter locations at the edge of the city will be able to be included in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting city bound ###\n",
    "\n",
    "for city in citynames_l:\n",
    "    folder = city\n",
    "    path = directory + connect + folder\n",
    "\n",
    "    #Check if folder exists and if not, create it\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # download/model a street network for some city\n",
    "    G = ox.graph_from_place(city, network_type=\"all\")\n",
    "\n",
    "    #Save to geopackage\n",
    "    ox.save_graph_geopackage(G, folder + \"/network.gpkg\")\n",
    "\n",
    "    #Save to graphml\n",
    "    ox.io.save_graphml(G, folder + \"/network.graphml\")\n",
    "    \n",
    "    # Retrieve only edges from the graph\n",
    "    nodes_proj, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
    "    \n",
    "    #Get the bounding box of all the edges, this will be the are of interest for each city\n",
    "    bbox_env = edges.unary_union.envelope\n",
    "\n",
    "    #Create buffer around city to make sure all count locations are captured\n",
    "    bbox_env_buffer = bbox_env.buffer(0.08, cap_style=3, join_style=2)\n",
    "\n",
    "    #Save bounding box as gpkg\n",
    "    envgdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries(bbox_env_buffer))\n",
    "    envgdf.to_file(city + connect + \"citybound_gpkg.gpkg\", driver=\"GPKG\")\n",
    "    \n",
    "    print(city + \" is finished\")\n",
    "\n",
    "    # Leave time in between extractions to save server connections\n",
    "    time.sleep(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download OSMNx point data\n",
    "\n",
    "Part of the variables that are being generated are statistics of POI points sorrounding each counter location. To calculate these statistics, the locations of these POI points are downloaded and saved in a city specific folder. The POI location points are extracted from the OSMnx API and saved for each indiviual city in the corresponding folder. \n",
    "\n",
    "For each city, the city bound files are used in order to determine the extent to which these POI points are meant to be downloaded. Each categorical collection of POI points is downloaded and saved as a GeoPackage file. \n",
    "\n",
    "Dependencies:\n",
    "1. City folders created\n",
    "2. City bounds downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downloading point data ###\n",
    "citynames_l = ['Glasgow City']\n",
    "for c in citynames_l:\n",
    "\n",
    "    #Get the correct directory to work in\n",
    "    city_path = directory + '/' + c\n",
    "\n",
    "    #Read the city bound that has been downloaded\n",
    "    citybound = gpd.read_file(city_path + '/citybound_gpkg.gpkg')\n",
    "\n",
    "    #Create shapely polygon from citybounds\n",
    "    for index, p in citybound.iterrows():\n",
    "        poly = p.geometry\n",
    "    \n",
    "    print(poly)\n",
    "\n",
    "    #Extracting bus stops\n",
    "    bus_stops = ox.geometries_from_polygon(poly, tags={'highway': 'bus_stop'})\n",
    "    bus_stops = bus_stops.to_crs(crs)\n",
    "    bus_stops = bus_stops[['geometry']]\n",
    "    bus_stops.to_file(city_path + \"/bus_stops.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting restaurants\n",
    "    restaurants = ox.geometries_from_polygon(poly, tags={'amenity': ['bar', 'pub', 'restaurant', 'cafe']})\n",
    "    restaurants = restaurants.to_crs(crs)\n",
    "    restaurants = restaurants[['geometry']]\n",
    "    restaurants.to_file(city_path + \"/restaurants.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting bike POIs\n",
    "    bikePOI = ox.geometries_from_polygon(poly, tags={'amenity': ['bicycle_parking', 'bicycle_repair_station', 'bicycle_rental']})\n",
    "    bikePOI.to_crs(crs)\n",
    "    print(bikePOI)\n",
    "    bikePOI = bikePOI[['geometry']]\n",
    "    bikePOI.to_file(city_path + \"/bikePOI.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting shops\n",
    "    shops = ox.geometries_from_polygon(poly, tags={'shop': ['department_store', 'supermarket', 'convenience']})\n",
    "    shops.to_crs(crs)\n",
    "    shops = shops[['geometry']]\n",
    "    shops.to_file(city_path + \"/shops.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting greenspace\n",
    "    greenspace = ox.geometries_from_polygon(poly, tags={'leisure': ['garden', 'nature_reserve', 'park', 'pitch']})\n",
    "    greenspace.to_crs(crs)\n",
    "    greenspace_cut = greenspace[['geometry']]\n",
    "    greenspace_cut.to_file(city_path + \"/greenspace.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting cyclelanes\n",
    "    cycleways = ox.geometries_from_polygon(poly, tags={'cycleway': True, 'highway':'cycleway'})\n",
    "    cycleways.to_crs(crs)\n",
    "    cycleways_c = cycleways[['geometry']]\n",
    "\n",
    "    #Create union out of all geometry extracted\n",
    "    temp_list = []\n",
    "\n",
    "    for index, x in cycleways_c.iterrows():\n",
    "        temp_list.append(x.geometry)\n",
    "\n",
    "    series = gpd.GeoSeries(temp_list)\n",
    "\n",
    "    #Convert to gdf and export as gpkg\n",
    "    cycleways_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries(series))\n",
    "    cycleways_gdf.to_file(city_path + \"/cycleways.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "    \n",
    "    print(c + \" done\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Google Earth Engine raster data\n",
    "\n",
    "The Google Earth Engine API is used in order to download global raster files for calculating additional geographical variables. The raster images are clipped down to the extent of each city using the city bounds. Once extracted, the images will have to be downloaded from Google Drive and inserted into each city folder in order to be able to be used for later calculation. \n",
    "\n",
    "IMPORTANT: Google Earth Engine is not able to download raster files to the device locally, it will therefore connect to the users Google Drive account and save the raster files there, which will have to then be manually downloaded and added into the folder corresponding to each of the cities.\n",
    "\n",
    "Dependencies:\n",
    "1. City bounds extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=9uvFxEkzH-Rgu3ZyW7LyuPBWXWlImK_SH-wDi6Y7d_U&tc=RS3M6mSUEhhOtBlM2ZSkOXssG_fCdeBCG951fn6UyVc&cc=aLD9Ma4shQndxnESROc2HqmI0vpZR-eHA2xFCSPvNvs>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=9uvFxEkzH-Rgu3ZyW7LyuPBWXWlImK_SH-wDi6Y7d_U&tc=RS3M6mSUEhhOtBlM2ZSkOXssG_fCdeBCG951fn6UyVc&cc=aLD9Ma4shQndxnESROc2HqmI0vpZR-eHA2xFCSPvNvs</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "#Authenticate Earth Engine through Google Account (this accounts Google Drive will be used to save the data)\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI done for Glasgow City\n",
      "GEE done for Glasgow City\n"
     ]
    }
   ],
   "source": [
    "### Raster file extraction from GEE ###\n",
    "\n",
    "for d in citynames_l:\n",
    "    \n",
    "    #Get the correct directory to work in\n",
    "    city_path = directory + '/' + d\n",
    "\n",
    "    #Read the city bound that has been downloaded\n",
    "    citybound = gpd.read_file(city_path + '/citybound_gpkg.gpkg')\n",
    "\n",
    "    #Create shapely polygon from citybounds\n",
    "    for index, p in citybound.iterrows():\n",
    "        poly = p.geometry\n",
    "\n",
    "    #Get citybound coordinates for extraction\n",
    "    xx, yy = poly.exterior.coords.xy\n",
    "    x = xx.tolist()\n",
    "    y = yy.tolist()\n",
    "    poly_coords = []\n",
    "\n",
    "    num = 0\n",
    "\n",
    "    for coord in x:\n",
    "        temp = []\n",
    "        temp.append(x[num])\n",
    "        temp.append(y[num])\n",
    "        poly_coords.append(temp)\n",
    "        num += 1\n",
    "\n",
    "    #Define region as ee.Geometry\n",
    "    region = ee.Geometry.Polygon(poly_coords)\n",
    "\n",
    "    ##NDVI##\n",
    "    #Downloading Sentinal 2 data\n",
    "    sen_2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').filterDate('2021-01-01', '2021-12-31').filterBounds(region).filterMetadata('CLOUDY_PIXEL_PERCENTAGE','less_than', 5)\n",
    "\n",
    "    sen_2 = sen_2.mean()\n",
    "    \n",
    "    #Calculating NDVI from image collection\n",
    "    def calculate_ndvi(image):\n",
    "        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "        return image.addBands(ndvi)\n",
    "\n",
    "    ndvi = calculate_ndvi(sen_2).select('NDVI')\n",
    "\n",
    "    #Export to GoogleDrive\n",
    "    task = ee.batch.Export.image.toDrive(**{\n",
    "        'image': ndvi,\n",
    "        'description': 'NVDI',\n",
    "        'folder': d,\n",
    "        'scale': 10,\n",
    "        'region': region.getInfo()['coordinates']\n",
    "        })\n",
    "\n",
    "    task.start()\n",
    "    print('NDVI done for ' + d)\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    ##DEM##\n",
    "    #Downloading Sentinal 2 data\n",
    "    srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "    srtm_s = srtm.select('elevation')\n",
    "\n",
    "    #Export to GoogleDrive\n",
    "    task = ee.batch.Export.image.toDrive(**{\n",
    "        'image': srtm_s,\n",
    "        'description': 'DEM',\n",
    "        'folder': d,\n",
    "        'scale': 30,\n",
    "        'region': region.getInfo()['coordinates']\n",
    "        })\n",
    "\n",
    "    task.start()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    ##LST##\n",
    "    #For the LST calculation, 3 raster files are needed. They are downloaded seperately and then used together in calculation.\n",
    "    #Might have to adjust cloud cover percentage for certain cities\n",
    "    #Downloading data\n",
    "    landsat_8 = ee.ImageCollection('LANDSAT/LC08/C02/T1').filterDate('2020-01-01', '2022-12-31').filterBounds(region).filterMetadata('CLOUD_COVER','less_than', 30)\n",
    "\n",
    "    #Select LST band from recording\n",
    "    red = landsat_8.select('B4')\n",
    "    nir = landsat_8.select('B5')\n",
    "    thermal = landsat_8.select('B10')\n",
    "\n",
    "    red_mean = red.mean()\n",
    "    nir_mean = nir.mean()\n",
    "    thermal_mean = thermal.mean()\n",
    "\n",
    "    #Export red to GoogleDrive\n",
    "    task = ee.batch.Export.image.toDrive(**{\n",
    "        'image': red_mean,\n",
    "        'description': 'LST_red',\n",
    "        'folder': d,\n",
    "        'scale': 30,\n",
    "        'region': region.getInfo()['coordinates']\n",
    "        })\n",
    "\n",
    "    task.start()\n",
    "\n",
    "    #Export nir to GoogleDrive\n",
    "    task = ee.batch.Export.image.toDrive(**{\n",
    "        'image': nir_mean,\n",
    "        'description': 'LST_nir',\n",
    "        'folder': d,\n",
    "        'scale': 30,\n",
    "        'region': region.getInfo()['coordinates']\n",
    "        })\n",
    "\n",
    "    task.start()\n",
    "\n",
    "    #Export red to GoogleDrive\n",
    "    task = ee.batch.Export.image.toDrive(**{\n",
    "        'image': thermal_mean,\n",
    "        'description': 'LST_thermal',\n",
    "        'folder': d,\n",
    "        'scale': 30,\n",
    "        'region': region.getInfo()['coordinates']\n",
    "        })\n",
    "\n",
    "    task.start()\n",
    "\n",
    "    # time.sleep(10)\n",
    "\n",
    "    ##Land Cover##\n",
    "    #Downloading Sentinal 2 data\n",
    "    lc_2 = ee.ImageCollection('ESA/WorldCover/v100').filterDate('2020-01-01', '2020-12-31').filterBounds(region).first()\n",
    "\n",
    "    #Select LST band from recording\n",
    "    lc = lc_2.select('Map')\n",
    "\n",
    "    #Export to GoogleDrive\n",
    "    task = ee.batch.Export.image.toDrive(**{\n",
    "        'image': lc,\n",
    "        'description': 'landcover',\n",
    "        'folder': d,\n",
    "        'scale': 30,\n",
    "        'region': region.getInfo()['coordinates']\n",
    "        })\n",
    "\n",
    "    task.start()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    ##Pop grid##\n",
    "    #Downloading WorldPop Global data\n",
    "    worldpop = ee.ImageCollection('WorldPop/GP/100m/pop').filterDate('2020-01-01', '2020-12-31').filterBounds(region)\n",
    "    \n",
    "    worldpop = worldpop.mean()\n",
    "    worldpop_2 = worldpop.select('population')\n",
    "\n",
    "    #Export to GoogleDrive\n",
    "    task = ee.batch.Export.image.toDrive(**{\n",
    "        'image': worldpop_2,\n",
    "        'description': 'population',\n",
    "        'folder': d,\n",
    "        'scale': 100,\n",
    "        'region': region.getInfo()['coordinates']\n",
    "        })\n",
    "\n",
    "    task.start()\n",
    "    print('GEE done for ' + d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting network statistics from OSMnx\n",
    "\n",
    "Part of the variables that are used in counter prediction are network statistics that are based on the OSMnx network sorrounding each of the counter locations. The below code extracts the networks around each of the counter locations and calculates the basic network statistics provided by OSMnx.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 631, 'm': 1589, 'k_avg': 5.0364500792393025, 'edge_length_total': 59534.31999999988, 'edge_length_avg': 37.46653241032088, 'streets_per_node_avg': 3.198098256735341, 'streets_per_node_counts': {0: 0, 1: 52, 2: 1, 3: 350, 4: 226, 5: 2}, 'streets_per_node_proportions': {0: 0.0, 1: 0.08240887480190175, 2: 0.001584786053882726, 3: 0.554675118858954, 4: 0.358161648177496, 5: 0.003169572107765452}, 'intersection_count': 579, 'street_length_total': 38075.68599999998, 'street_segment_count': 964, 'street_length_avg': 39.49759958506222, 'circuity_avg': 1.0486865710522633, 'self_loop_proportion': 0.002074688796680498}\n",
      "{'n': 226, 'm': 533, 'k_avg': 4.716814159292035, 'edge_length_total': 18677.141000000003, 'edge_length_avg': 35.041540337711076, 'streets_per_node_avg': 2.7389380530973453, 'streets_per_node_counts': {0: 0, 1: 54, 2: 0, 3: 126, 4: 43, 5: 3}, 'streets_per_node_proportions': {0: 0.0, 1: 0.23893805309734514, 2: 0.0, 3: 0.5575221238938053, 4: 0.1902654867256637, 5: 0.01327433628318584}, 'intersection_count': 172, 'street_length_total': 10661.291999999992, 'street_segment_count': 297, 'street_length_avg': 35.89660606060603, 'circuity_avg': 1.0292498466565638, 'self_loop_proportion': 0.0}\n"
     ]
    }
   ],
   "source": [
    "### Extracting network statistics ###\n",
    "\n",
    "df_keys = []\n",
    "df_values = []\n",
    "df_index = []\n",
    "locations = []\n",
    "key_saved = False\n",
    "\n",
    "#Select which counter points are meant to be extracted\n",
    "df_select = df.iloc[1:3]\n",
    "\n",
    "for index, row in df_select.iterrows():\n",
    "    lat = row['latitude']\n",
    "    lon = row['longitude']\n",
    "    point = (lat, lon)\n",
    "\n",
    "    #Download network graph for area\n",
    "    G = ox.graph_from_point(point, dist=500, network_type=\"all\", clean_periphery=True)\n",
    "    # G_proj = ox.project_graph(G)\n",
    "\n",
    "    #Calculate statistics around the particular point\n",
    "    stats = ox.basic_stats(G)\n",
    "\n",
    "    print(stats)\n",
    "#     keys = list(stats.keys())\n",
    "#     values = list(stats.values())\n",
    "#     locations.append(row['name'])\n",
    "\n",
    "#     if key_saved == False:\n",
    "#         df_keys = keys\n",
    "#         key_saved = True\n",
    "\n",
    "#     df_values.append(values)\n",
    "#     df_index.append(index)\n",
    "\n",
    "#     time.sleep(20)\n",
    "    \n",
    "#     (print('location', index, 'done'))\n",
    "\n",
    "# df1 = pd.DataFrame(df_values, columns=df_keys)\n",
    "# df1['locations'] = locations\n",
    "\n",
    "# df1\n",
    "\n",
    "inter3 = []\n",
    "inter4 = []\n",
    "\n",
    "for x in intersect_count:\n",
    "    if type(x) is int:\n",
    "        in_3 = 0\n",
    "        in_4 = 0\n",
    "    else:    \n",
    "        if 3 in x.keys():    \n",
    "            in_3 = x[3]\n",
    "        else:\n",
    "            in_3 = 0\n",
    "        if 4 in x.keys():\n",
    "            in_4 = x[4]\n",
    "        else:\n",
    "            in_4 = 0\n",
    "    inter3.append(in_3)\n",
    "    inter4.append(in_4)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['3_way_int_count'] = inter3\n",
    "data['4_way_int_count'] = inter4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results to a csv in directory\n",
    "df1.to_csv(\"network_statistics_4.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create buffers\n",
    "\n",
    "In order to calculate the statistics of the various spatial variables extracted above, buffers are created around each counter location. These buffers are then saved as a geopackage file in each city folder in order to be extracted for calculating the values for the different spatial variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create buffers ###\n",
    "\n",
    "for e in citynames_l:\n",
    "\n",
    "    #Define path\n",
    "    city_path = directory + connect + e\n",
    "\n",
    "    df2 = df[df['city'] == e]\n",
    "    df3 = df2[['name', 'latitude', 'longitude']].copy()\n",
    "\n",
    "    #Convert to geopandas with point \n",
    "    geometry = [Point(xy) for xy in zip(df3.longitude, df3.latitude)]\n",
    "    df4 = df3.drop(['longitude', 'latitude'], axis=1)\n",
    "    gdf = gpd.GeoDataFrame(df4, crs=\"EPSG:4326\", geometry=geometry)\n",
    "    gdf = gdf.to_crs(crs)\n",
    "\n",
    "    ##Create buffers##\n",
    "    buffer = gdf.buffer(370) #buffer zones end up being 500m diameter\n",
    "    buffer_gdf = gpd.GeoDataFrame(geometry=buffer).reset_index().to_crs(4326)\n",
    "\n",
    "    #Save gdf of buffers to folder\n",
    "    folder_name = \"buffer_gpkg_1\"\n",
    "    folder_path = directory + connect + e + connect + folder_name\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    buffer_gdf.to_file(folder_path + \"/point_buffers.gpkg\", driver=\"GPKG\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating point variables\n",
    "\n",
    "These functions use the POI points extracted from OSMnx API to calculate the amount that each POI occurs around each counter location. This calculation loads both the buffers and OSMnx point location to calculate the variables. \n",
    "\n",
    "Dependencies:\n",
    "1. Buffers created\n",
    "2. OSMnx point data downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate point variables ###\n",
    "\n",
    "#Check for overlap between each polygon and points\n",
    "bike_point_list = []\n",
    "bus_stops_list = []\n",
    "restaurant_list = []\n",
    "shops_list = []\n",
    "\n",
    "for city in citynames_l:\n",
    "\n",
    "    #Defining path for current city\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Connect folder and load in points\n",
    "    bike_shops = gpd.read_file(city_path + \"/bikePOI.gpkg\").to_crs(crs)\n",
    "    bus_stops = gpd.read_file(city_path + \"/bus_stops.gpkg\").to_crs(crs)\n",
    "    restaurants = gpd.read_file(city_path + \"/restaurants.gpkg\").to_crs(crs)\n",
    "    shops = gpd.read_file(city_path + \"/shops.gpkg\").to_crs(crs)\n",
    "\n",
    "    #Load in buffers\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(crs)\n",
    "\n",
    "    for index, row in buffer_gdkg.iterrows():\n",
    "        temp = row.geometry\n",
    "        bike_point_counts = 0\n",
    "        bus_stop_counts = 0\n",
    "        restaurant_counts = 0\n",
    "        shop_counts = 0\n",
    "\n",
    "        #overlap bike shops\n",
    "        for index, row in bike_shops.iterrows():\n",
    "            temp1 = row.geometry\n",
    "            \n",
    "            if temp.contains(temp1):\n",
    "                bike_point_counts += 1\n",
    "\n",
    "        #overlap bus stops\n",
    "        for index, row in bus_stops.iterrows():\n",
    "            temp1 = row.geometry\n",
    "            \n",
    "            if temp.contains(temp1):\n",
    "                bus_stop_counts += 1\n",
    "\n",
    "        #overlap restaurants\n",
    "        for index, row in restaurants.iterrows():\n",
    "            temp1 = row.geometry\n",
    "            \n",
    "            if temp.contains(temp1):\n",
    "                restaurant_counts += 1\n",
    "\n",
    "        #overlap shops\n",
    "        for index, row in shops.iterrows():\n",
    "            temp1 = row.geometry\n",
    "            \n",
    "            if temp.contains(temp1):\n",
    "                shop_counts += 1\n",
    "\n",
    "        bike_point_list.append(bike_point_counts)\n",
    "        bus_stops_list.append(bus_stop_counts)\n",
    "        restaurant_list.append(restaurant_counts)\n",
    "        shops_list.append(shop_counts)\n",
    "\n",
    "\n",
    "df5 = pd.DataFrame()\n",
    "\n",
    "df5['bike_points'] = bike_point_list\n",
    "df5['bus_stops'] = bus_stops_list\n",
    "df5['restaurants'] = restaurant_list\n",
    "df5['shop_list'] = shops_list\n",
    "\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv\n",
    "df5.to_csv('point_data_export_2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating distance to POI variables\n",
    "\n",
    "Using the greenspace and education POI locations, this algorithm calculates the network distance between each counter location and their closest greenspace and education POI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating distance variabels to POIs ###\n",
    "\n",
    "greenspace_dist_list = []\n",
    "bike_dist_list = []\n",
    "\n",
    "for city in citynames_l:\n",
    "\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Creating dataframe\n",
    "    df2 = df[df['city'] == city]\n",
    "    df3 = df2[['name', 'latitude', 'longitude']].copy()\n",
    "\n",
    "    #Convert to geopandas with point \n",
    "    geometry = [Point(xy) for xy in zip(df3.longitude, df3.latitude)]\n",
    "    df4 = df3.drop(['longitude', 'latitude'], axis=1)\n",
    "    gdf = gpd.GeoDataFrame(df4, crs=\"EPSG:4326\", geometry=geometry)\n",
    "    gdf = gdf.to_crs(crs)\n",
    "\n",
    "    #Load in street network graph\n",
    "    G = ox.load_graphml(city_path + \"/network.graphml\")\n",
    "\n",
    "    #Load in greenspace polygons\n",
    "    greenspace = gpd.read_file(city_path + \"/greenspace.gpkg\").to_crs(crs)\n",
    "\n",
    "    for index, point in gdf.iterrows():\n",
    "        #Find closest polygon and get centroid\n",
    "        polygon_index = greenspace.distance(point.geometry).sort_values().index[0]\n",
    "        nearest_centroid = greenspace.loc[polygon_index].geometry.centroid\n",
    "\n",
    "        #Extract coordinates of point\n",
    "        x, y = point.geometry.coords.xy\n",
    "        x = x[0]\n",
    "        y = y[0]\n",
    "\n",
    "        #Find closest node\n",
    "        node_counter = ox.distance.nearest_nodes(G, x, y)\n",
    "        \n",
    "        #Extract coordinates of point\n",
    "        xx, yy = nearest_centroid.xy\n",
    "        xx = xx[0]\n",
    "        yy = yy[0]\n",
    "\n",
    "        #Find closest node\n",
    "        node_greenspace = ox.distance.nearest_nodes(G, xx, yy)\n",
    "\n",
    "        #Calculate shortest path\n",
    "        try:\n",
    "            nx.shortest_path_length(G, node_counter, node_greenspace)\n",
    "        \n",
    "        except:\n",
    "            s_path = 0\n",
    "        \n",
    "        else:\n",
    "            s_path = nx.shortest_path_length(G, node_counter, node_greenspace)\n",
    "        \n",
    "        greenspace_dist_list.append(s_path)\n",
    "\n",
    "    #Load in street network graph\n",
    "    G = ox.load_graphml(city_path + \"/network.graphml\")\n",
    "\n",
    "    #Load in greenspace polygons\n",
    "    bikePOI = gpd.read_file(city_path + \"/bikePOI.gpkg\").to_crs(crs)\n",
    "\n",
    "    #Import points as gdf and loop through them\n",
    "    for index, point in gdf.iterrows():\n",
    "        polygon_index = bikePOI.distance(point.geometry).sort_values().index[0]\n",
    "        nearest_bike = bikePOI.loc[polygon_index].geometry.centroid\n",
    "\n",
    "        #Extract coordinates of point\n",
    "        x, y = point.geometry.coords.xy\n",
    "        x = x[0]\n",
    "        y = y[0]\n",
    "\n",
    "        #Find closest node\n",
    "        node_counter = ox.distance.nearest_nodes(G, x, y)\n",
    "        \n",
    "        #Extract coordinates of point\n",
    "        xx, yy = nearest_bike.xy\n",
    "        xx = xx[0]\n",
    "        yy = yy[0]\n",
    "        \n",
    "        #Find closest node\n",
    "        node_greenspace = ox.distance.nearest_nodes(G, xx, yy)\n",
    "\n",
    "        #Calculate shortest path\n",
    "        try:\n",
    "            nx.shortest_path_length(G, node_counter, node_greenspace)\n",
    "        \n",
    "        except:\n",
    "            s_path = 0\n",
    "        \n",
    "        else:\n",
    "            s_path = nx.shortest_path_length(G, node_counter, node_greenspace)\n",
    "\n",
    "        bike_dist_list.append(s_path)\n",
    "\n",
    "    print(city + \" is done\")\n",
    "\n",
    "df6 = pd.DataFrame()\n",
    "df6['dist_to_greenspace'] = greenspace_dist_list\n",
    "df6['dist_to_bikePOI'] = bike_dist_list\n",
    "\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv\n",
    "df6.to_csv('dist_to_green_bike.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the total length of cyclepath in buffer\n",
    "\n",
    "Using the downloaded cycling path network, this function calculates the total length of cycling infrastructure found in te buffer around each counter location. \n",
    "\n",
    "Dependencies:\n",
    "1. Buffers created\n",
    "2. Cycle infrastructure downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate total length of cyclepaths ###\n",
    "\n",
    "total_cycle_list = []\n",
    "\n",
    "for city in citynames_l:    \n",
    "    \n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Load in cycling network data and buffer data\n",
    "    cycleways = gpd.read_file(city_path + \"/cycleways.gpkg\").to_crs(crs)\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(crs)\n",
    "\n",
    "    for index, buffer in buffer_gdkg.iterrows():\n",
    "        clip = cycleways.clip(buffer.geometry)\n",
    "        total_cycle = clip.length.sum()\n",
    "        \n",
    "        total_cycle_list.append(total_cycle)\n",
    "\n",
    "df7  = pd.DataFrame()\n",
    "df7['cycle_length'] = total_cycle_list\n",
    "\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv\n",
    "df7.to_csv('cycle_length_2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and calculate building area\n",
    "\n",
    "This code extracts the buildings in the buffer around each counter location and calculates that total amount of area that the buildings cover within each buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract and calculate building area ###\n",
    "build_dens_list = []\n",
    "\n",
    "#Extracting buildings and calculating building density around points\n",
    "for city in citynames_l:\n",
    "\n",
    "    city_path = directory + connect + city\n",
    "    \n",
    "    #Load in buffers\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(4326)\n",
    "\n",
    "    for index, row in buffer_gdkg.iterrows():\n",
    "        poly = row.geometry\n",
    "\n",
    "        #Extracting buildings to calculate building density\n",
    "        buildings = ox.geometries_from_polygon(poly, tags={'building': True})\n",
    "        buildings = buildings.to_crs(3857)\n",
    "\n",
    "        temp_l = []\n",
    "\n",
    "        for index, x in buildings.iterrows():\n",
    "            temp_l.append(x.geometry)\n",
    "\n",
    "        b_series = gpd.GeoSeries(temp_l).area\n",
    "        b_total_area = b_series.sum()\n",
    "        build_dens_list.append(b_total_area)\n",
    "\n",
    "    print(city + ' done')\n",
    "\n",
    "\n",
    "df8 = pd.DataFrame()\n",
    "df8['build_area'] = build_dens_list\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv\n",
    "df8.to_csv('building.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate streets per node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amsterdam\n",
      "Rotterdam\n",
      "New York\n",
      "Seattle\n",
      "Utrecht\n",
      "Norfolk, Virginia\n",
      "Arlington County\n",
      "Austin\n",
      "Minneapolis\n",
      "Almere\n",
      "Arnhem\n",
      "London\n",
      "Northampton\n",
      "Leeds\n",
      "Birmingham\n",
      "Nijmegen\n",
      "Apeldoorn\n",
      "York\n",
      "Duluth\n",
      "Maastricht\n",
      "Boulder\n",
      "Portland\n",
      "Cambridge\n",
      "Denver\n",
      "Philadelphia\n",
      "Houston\n",
      "Dallas\n",
      "Glasgow City\n",
      "Edinburgh\n",
      "Aberdeen\n",
      "Wageningen\n",
      "Gouda\n",
      "San Diego\n",
      "Venlo\n",
      "Amersfoort\n",
      "Tampa\n",
      "Gainesville\n",
      "Tallahassee\n",
      "Fort Lauderdale\n",
      "Miami\n",
      "The Hague\n",
      "Leiden\n",
      "Zeist\n",
      "Dordrecht\n",
      "Ede\n",
      "Raleigh\n",
      "Cary\n",
      "Durham\n",
      "Greensboro\n",
      "Charlotte\n"
     ]
    }
   ],
   "source": [
    "intersect_count = []\n",
    "\n",
    "for city in citynames_l:    \n",
    "    \n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Load in street network data and buffer data\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(4326)\n",
    "\n",
    "    for polygon in buffer_gdkg['geometry']:\n",
    "        try:\n",
    "            G = ox.graph_from_polygon(polygon, network_type='drive')\n",
    "            node_count = ox.stats.streets_per_node_counts(G)\n",
    "        except:\n",
    "            node_count = 0\n",
    "        intersect_count.append(node_count)\n",
    "\n",
    "    print(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('3_4_int.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate LST and save as new TIF file\n",
    "\n",
    "This code takes the extracted Landsat 8 raster files in order to calculate a new raster file containing the Land Surface Temperature for each city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate LST layer###\n",
    "\n",
    "for city in citynames_l:\n",
    "    \n",
    "    city_path = directory + '/' + city\n",
    "\n",
    "    #Load in each band that was exported from GEE\n",
    "    red = rasterio.open(city_path + \"/LST_red.tif\")\n",
    "    nir = rasterio.open(city_path + \"/LST_nir.tif\")\n",
    "    thermal = rasterio.open(city_path + \"/LST_thermal.tif\")\n",
    "\n",
    "    #Read the bands to be used for calculation\n",
    "    redImage = red.read(1).astype('f4')\n",
    "    nirImage = nir.read(1).astype('f4')\n",
    "    thermalImage = thermal.read(1).astype('f4')\n",
    "\n",
    "    lst_image_single_window = single_window(thermalImage, redImage, nirImage, unit='celcius')\n",
    "\n",
    "    #Define affine transformation\n",
    "    affine = red.transform\n",
    "\n",
    "    #Create new raster file with calculated variables\n",
    "    with rasterio.open(\n",
    "        city_path + \"/LST_calculated.tif\",\n",
    "        mode=\"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=lst_image_single_window.shape[0],\n",
    "        width=lst_image_single_window.shape[1],\n",
    "        count=1,\n",
    "        dtype=lst_image_single_window.dtype,\n",
    "        crs=4326,\n",
    "        transform=affine,\n",
    "                        ) as new_dataset:\n",
    "            new_dataset.write(lst_image_single_window, 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate raster statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate raster statistics ###\n",
    "\n",
    "#Define lists for each variable\n",
    "ndvi_mean_lst = []\n",
    "ndvi_std_lst = []\n",
    "dem_mean_lst = []\n",
    "dem_std_lst = []\n",
    "lst_mean_lst = []\n",
    "lst_std_lst = []\n",
    "lc_entropy_lst = []\n",
    "pop_mean_lst = []\n",
    "pop_std_lst = []\n",
    "\n",
    "def entropy(x):\n",
    "    unique = np.unique(x, return_counts=True)\n",
    "    unique_count = len(unique[0])\n",
    "    unique_size = unique[1]\n",
    "    total_size = x.size\n",
    "    \n",
    "    total = 0.0\n",
    "\n",
    "    for y in unique_size:\n",
    "        r = (y/total_size)*np.log(y/total_size)\n",
    "        total = r + total\n",
    "\n",
    "    c_entropy = (total/np.log(unique_count))*-1\n",
    "\n",
    "    return c_entropy\n",
    "\n",
    "for city in citynames_l:\n",
    "\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Load in the buffers around the counter locations\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(4326)\n",
    "\n",
    "    ##NDVI##\n",
    "    #Load in population file\n",
    "    ndvi = rasterio.open(city_path + \"/NVDI.tif\")\n",
    "    arr = ndvi.read(1)\n",
    "    affine = ndvi.transform\n",
    "\n",
    "    statistics = zonal_stats(buffer_gdkg, arr, affine=affine, stats=['mean', 'std'])\n",
    "\n",
    "    for x in statistics:\n",
    "        ndvi_mean = x['mean']\n",
    "        ndvi_std = x['std']\n",
    "\n",
    "        ndvi_mean_lst.append(ndvi_mean)\n",
    "        ndvi_std_lst.append(ndvi_std)\n",
    "\n",
    "    ##DEM##\n",
    "    #Load in population file\n",
    "    dem = rasterio.open(city_path + \"/DEM.tif\")\n",
    "    arr = dem.read(1)\n",
    "    affine = dem.transform\n",
    "\n",
    "    statistics = zonal_stats(buffer_gdkg, arr, affine=affine, stats=['mean', 'std'])\n",
    "\n",
    "    for x in statistics:\n",
    "        dem_mean = x['mean']\n",
    "        dem_std = x['std']\n",
    "\n",
    "        dem_mean_lst.append(dem_mean)\n",
    "        dem_std_lst.append(dem_std)\n",
    "\n",
    "\n",
    "    ##LST##\n",
    "    #Load in lst files\n",
    "    lst = rasterio.open(city_path + \"/LST_calculated.tif\")\n",
    "    arr = lst.read(1)\n",
    "    affine = lst.transform\n",
    "\n",
    "    # #Load in buffer file\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(4326)\n",
    "\n",
    "    statistics = zonal_stats(buffer_gdkg, arr, affine=affine, stats=['mean', 'std'])\n",
    "\n",
    "    for x in statistics:\n",
    "        lst_mean = x['mean']\n",
    "        lst_std = x['std']\n",
    "\n",
    "        lst_mean_lst.append(lst_mean)\n",
    "        lst_std_lst.append(lst_std)\n",
    "\n",
    "    \n",
    "    ##Populationn density##\n",
    "    #Load in population files\n",
    "    pop = rasterio.open(city_path + \"/population.tif\")\n",
    "    arr = pop.read(1)\n",
    "    affine = pop.transform\n",
    "\n",
    "    # #Load in buffer file\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(4326)\n",
    "\n",
    "    statistics = zonal_stats(buffer_gdkg, arr, affine=affine, stats=['mean', 'std'])\n",
    "\n",
    "    for x in statistics:\n",
    "        pop_mean = x['mean']\n",
    "        pop_std = x['std']\n",
    "\n",
    "        pop_mean_lst.append(pop_mean)\n",
    "        pop_std_lst.append(pop_std)\n",
    "\n",
    "    \n",
    "    ##Land Cover entropy##\n",
    "    lc = rasterio.open(city_path + \"/landcover.tif\")\n",
    "    arr = lc.read(1)\n",
    "    affine = lc.transform\n",
    "\n",
    "    # #Load in buffer file\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(4326)\n",
    "\n",
    "    statistics = zonal_stats(buffer_gdkg, arr, affine=affine, stats=['majority'], add_stats={'entropy':entropy})\n",
    "\n",
    "    for x in statistics:\n",
    "        lc_entropy = x['entropy']\n",
    "        lc_entropy_lst.append(lc_entropy)\n",
    "\n",
    "#Add all variables to dataframe\n",
    "df9 = pd.DataFrame()\n",
    "df9['ndvi_mean'] = ndvi_mean_lst\n",
    "df9['ndvi_std'] = ndvi_std_lst\n",
    "df9['dem_mean'] = dem_mean_lst\n",
    "df9['dem_std'] = dem_std_lst\n",
    "df9['lst_mean'] = lst_mean_lst\n",
    "df9['lst_std'] = lst_std_lst\n",
    "df9['pop_mean'] = pop_mean_lst\n",
    "df9['pop_std'] = pop_std_lst\n",
    "df9['lc_entropy'] = lc_entropy_lst\n",
    "\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv\n",
    "df9.to_csv('raster_calculations.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e04e9d70aa0476d320a89e4247bc687a844ab51af6dd849df37511de572e0dcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

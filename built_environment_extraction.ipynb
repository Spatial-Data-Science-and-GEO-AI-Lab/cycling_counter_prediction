{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing needed libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import ee\n",
    "import time\n",
    "import rasterio\n",
    "import math\n",
    "import pyproj\n",
    "import libpysal\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.geometry\n",
    "from rasterio.warp import reproject\n",
    "from rasterstats import zonal_stats\n",
    "from pylandtemp import single_window\n",
    "from shapely.geometry import Point, Polygon, LineString"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting initial varibles\n",
    "\n",
    "Adjust the variables below in order to fine tune the calculations done throughout the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the directory for download and directory of counter file\n",
    "os.chdir('/Users/winke/Documents/University/Thesis/Extracts')\n",
    "\n",
    "#Read the counter data into the program\n",
    "df = pd.read_csv(\"/Users/winke/Documents/University/Thesis/Extracts/weekly_new.csv\")\n",
    "\n",
    "#Checking working directory and creating path\n",
    "directory = os.getcwd() #use working directory or desired location\n",
    "connect = \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select cities and create a list that can be iterated through\n",
    "citynames_df = df[['city', 'espg']]\n",
    "citynames_df = citynames_df.drop_duplicates()\n",
    "citynames_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate ESPG values for CSV \n",
    "Calculate the ESPG for each city in order to have the correct projection when working with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the function to conver the coordinates into UTM coordinates\n",
    "def convert_wgs_to_utm(lon, lat):\n",
    "    utm_band = str((math.floor((lon + 180) / 6 ) % 60) + 1)\n",
    "    if len(utm_band) == 1:\n",
    "        utm_band = '0'+utm_band\n",
    "    if lat >= 0:\n",
    "        epsg_code = '326' + utm_band\n",
    "    else:\n",
    "        epsg_code = '327' + utm_band\n",
    "    return epsg_code\n",
    "\n",
    "#Loop through all coordinates and establish ESPG\n",
    "espg_list = []\n",
    "lat = df['latitude'].to_list()\n",
    "lon = df['longitude'].to_list()\n",
    "\n",
    "for coord in zip(lat, lon):\n",
    "    espg = convert_wgs_to_utm(coord[1], coord[0])\n",
    "\n",
    "    espg_list.append(espg)\n",
    "\n",
    "#Add onto existing dataframe\n",
    "df1 = df.copy()\n",
    "df1['espg'] = espg_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating city folders and extracting city bounds\n",
    "\n",
    "In order to be able to extract geographic variables for each of the cities included in the analysis, the bounds of the city will have to be determined. The bounds are extracted from the OSMnx API and converted into a rectangular shape, ecompassing the total extent of the city, as defined by OSMnx. Additionally, a small buffer is added to ensure that counter locations at the edge of the city will be able to be included in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Extracting city bound ###\n",
    "\n",
    "for index, row in citynames_df.iterrows():\n",
    "    \n",
    "    city = row['city']\n",
    "    path = directory + connect + city\n",
    "\n",
    "    #Check if folder exists and if not, create it\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # download/model a street network for some city\n",
    "    G = ox.graph_from_place(city, network_type=\"drive\")\n",
    "\n",
    "    #Save to geopackage\n",
    "    ox.save_graph_geopackage(G, city + \"/network.gpkg\")\n",
    "\n",
    "    #Save to graphml\n",
    "    ox.io.save_graphml(G, city + \"/network.graphml\")\n",
    "    \n",
    "    # Retrieve only edges from the graph\n",
    "    nodes_proj, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
    "    \n",
    "    #Get the bounding box of all the edges, this will be the are of interest for each city\n",
    "    bbox_env = edges.unary_union.envelope\n",
    "\n",
    "    #Create buffer around city to make sure all count locations are captured\n",
    "    bbox_env_buffer = bbox_env.buffer(0.08, cap_style=3, join_style=2)\n",
    "\n",
    "    #Save bounding box as gpkg\n",
    "    envgdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries(bbox_env_buffer))\n",
    "    envgdf.to_file(city + connect + \"citybound_gpkg.gpkg\", driver=\"GPKG\")\n",
    "    \n",
    "    print(city + \" is finished\")\n",
    "\n",
    "    # Leave time in between extractions to save server connections\n",
    "    time.sleep(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download OSMNx point data\n",
    "\n",
    "Part of the variables that are being generated are statistics of POI points sorrounding each counter location. To calculate these statistics, the locations of these POI points are downloaded and saved in a city specific folder. The POI location points are extracted from the OSMnx API and saved for each indiviual city in the corresponding folder. \n",
    "\n",
    "For each city, the city bound files are used in order to determine the extent to which these POI points are meant to be downloaded. Each categorical collection of POI points is downloaded and saved as a GeoPackage file. \n",
    "\n",
    "Dependencies:\n",
    "1. City folders created\n",
    "2. City bounds downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Downloading point data ###\n",
    "for index, row in citynames_df.iterrows():\n",
    "    city = row['city']\n",
    "    espg = row['espg']\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Read the city bound that has been downloaded\n",
    "    citybound = gpd.read_file(city_path + '/citybound_gpkg.gpkg')\n",
    "\n",
    "    #Create shapely polygon from citybounds\n",
    "    for index, p in citybound.iterrows():\n",
    "        poly = p.geometry\n",
    "    \n",
    "    #Extracting bus stops\n",
    "    bus_stops = ox.geometries_from_polygon(poly, tags={'highway': 'bus_stop'})\n",
    "    bus_stops = bus_stops.to_crs(espg)\n",
    "    bus_stops = bus_stops[['geometry']]\n",
    "    bus_stops.to_file(city_path + \"/bus_stops.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting restaurants\n",
    "    restaurants = ox.geometries_from_polygon(poly, tags={'amenity': ['bar', 'pub', 'restaurant', 'cafe']})\n",
    "    restaurants = restaurants.to_crs(espg)\n",
    "    restaurants = restaurants[['geometry']]\n",
    "    restaurants.to_file(city_path + \"/restaurants.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting bike POIs\n",
    "    bikePOI = ox.geometries_from_polygon(poly, tags={'amenity': ['bicycle_parking', 'bicycle_repair_station', 'bicycle_rental']})\n",
    "    bikePOI.to_crs(espg)\n",
    "    bikePOI = bikePOI[['geometry']]\n",
    "    bikePOI.to_file(city_path + \"/bikePOI.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting daily shops\n",
    "    shops = ox.geometries_from_polygon(poly, tags={'shop': ['department_store', 'supermarket', 'convenience']})\n",
    "    shops.to_crs(espg)\n",
    "    shops = shops[['geometry']]\n",
    "    shops.to_file(city_path + \"/daily_shops.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting business shops\n",
    "    shops_2 = ox.geometries_from_polygon(poly, tags={'shop': ['clothes', 'jewelry', 'shoes', 'tailor', 'beauty', 'cosmetics', 'hairdresser',\n",
    "                                                    'doityourself', 'garden_center', 'hardware', 'mall', 'department_store']})\n",
    "    shops_2.to_crs(espg)\n",
    "    shops_2 = shops_2[['geometry']]\n",
    "    shops_2.to_file(city_path + \"/business_shops.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    # time.sleep(10)\n",
    "\n",
    "    #Extracting traffic signals\n",
    "    tfs = ox.geometries_from_polygon(poly, tags={'highway': 'traffic_signals'})\n",
    "    tfs.to_crs(espg)\n",
    "    tfs = tfs[['geometry']]\n",
    "    tfs.to_file(city_path + \"/traffic_signals.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting greenspace\n",
    "    greenspace = ox.geometries_from_polygon(poly, tags={'leisure': ['garden', 'nature_reserve', 'park', 'pitch']})\n",
    "    greenspace.to_crs(espg)\n",
    "    greenspace_cut = greenspace[['geometry']]\n",
    "    greenspace_cut.to_file(city_path + \"/greenspace.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting eduPOI\n",
    "    edu_POI = ox.geometries_from_polygon(poly, tags={'amenity': ['school', 'university', 'college']})\n",
    "    edu_POI.to_crs(espg)\n",
    "    edu_POI = edu_POI[['geometry']]\n",
    "    edu_POI.to_file(city_path + \"/eduPOI.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting trainstations\n",
    "    station = ox.geometries_from_polygon(poly, tags={'railway': ['station']})\n",
    "    station.to_crs(espg)\n",
    "    station = station[['geometry']]\n",
    "    station.to_file(city_path + \"/t_stations.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    #Extracting cyclelanes\n",
    "    cycleways = ox.geometries_from_polygon(poly, tags={'cycleway': True, 'highway':'cycleway'})\n",
    "    cycleways.to_crs(espg)\n",
    "    cycleways_c = cycleways[['geometry']]\n",
    "\n",
    "    #Create union out of all geometry extracted\n",
    "    temp_list = []\n",
    "\n",
    "    for index, x in cycleways_c.iterrows():\n",
    "        temp_list.append(x.geometry)\n",
    "\n",
    "    series = gpd.GeoSeries(temp_list)\n",
    "\n",
    "    #Convert to gdf and export as gpkg\n",
    "    cycleways_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries(series))\n",
    "    cycleways_gdf.to_file(city_path + \"/cycleways.gpkg\", driver=\"GPKG\")\n",
    "    \n",
    "    print(city + \" done\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Google Earth Engine raster data\n",
    "\n",
    "The Google Earth Engine API is used in order to download global raster files for calculating additional geographical variables. The raster images are clipped down to the extent of each city using the city bounds. Once extracted, the images will have to be downloaded from Google Drive and inserted into each city folder in order to be able to be used for later calculation. \n",
    "\n",
    "IMPORTANT: Google Earth Engine is not able to download raster files to the device locally, it will therefore connect to the users Google Drive account and save the raster files there, which will have to then be manually downloaded and added into the folder corresponding to each of the cities.\n",
    "\n",
    "Dependencies:\n",
    "1. City bounds extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authenticate Earth Engine through Google Account (this accounts Google Drive will be used to save the data)\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Raster file extraction from GEE ###\n",
    "\n",
    "for index, row in citynames_df.iterrows():\n",
    "    \n",
    "    city = row['city']\n",
    "    espg = row['espg']\n",
    "    year = str(row['year'])\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Read the city bound that has been downloaded\n",
    "    citybound = gpd.read_file(city_path + '/citybound_gpkg.gpkg')\n",
    "\n",
    "    #Create shapely polygon from citybounds\n",
    "    for index, p in citybound.iterrows():\n",
    "        poly = p.geometry\n",
    "\n",
    "    #Get citybound coordinates for extraction\n",
    "    xx, yy = poly.exterior.coords.xy\n",
    "    x = xx.tolist()\n",
    "    y = yy.tolist()\n",
    "    poly_coords = []\n",
    "\n",
    "    num = 0\n",
    "\n",
    "    for coord in x:\n",
    "        temp = []\n",
    "        temp.append(x[num])\n",
    "        temp.append(y[num])\n",
    "        poly_coords.append(temp)\n",
    "        num += 1\n",
    "\n",
    "    #Define region as ee.Geometry\n",
    "    region = ee.Geometry.Polygon(poly_coords)\n",
    "\n",
    "    # ##NDVI##\n",
    "    # #Downloading Sentinal 2 data\n",
    "    # sen_2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').filterDate('2021-01-01', '2021-12-31').filterBounds(region).filterMetadata('CLOUDY_PIXEL_PERCENTAGE','less_than', 5)\n",
    "\n",
    "    # sen_2 = sen_2.mean()\n",
    "    \n",
    "    # #Calculating NDVI from image collection\n",
    "    # def calculate_ndvi(image):\n",
    "    #     ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    #     return image.addBands(ndvi)\n",
    "\n",
    "    # ndvi = calculate_ndvi(sen_2).select('NDVI')\n",
    "\n",
    "    # #Export to GoogleDrive\n",
    "    # task = ee.batch.Export.image.toDrive(**{\n",
    "    #     'image': ndvi,\n",
    "    #     'description': 'NVDI',\n",
    "    #     'folder': city,\n",
    "    #     'scale': 10,\n",
    "    #     'crs': 'EPSG:' + str(espg),\n",
    "    #     'region': region.getInfo()['coordinates']\n",
    "    #     })\n",
    "\n",
    "    # task.start()\n",
    "\n",
    "    # time.sleep(10)\n",
    "\n",
    "    # ##DEM##\n",
    "    # #Downloading Sentinal 2 data\n",
    "    # srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "    # srtm_s = srtm.select('elevation')\n",
    "\n",
    "    # #Export to GoogleDrive\n",
    "    # task = ee.batch.Export.image.toDrive(**{\n",
    "    #     'image': srtm_s,\n",
    "    #     'description': 'DEM',\n",
    "    #     'folder': city,\n",
    "    #     'scale': 30,\n",
    "    #     'crs': 'EPSG:' + str(espg),\n",
    "    #     'region': region.getInfo()['coordinates']\n",
    "    #     })\n",
    "\n",
    "    # task.start()\n",
    "\n",
    "    # time.sleep(10)\n",
    "\n",
    "    ##LST##\n",
    "    #For the LST calculation, 3 raster files are needed. They are downloaded seperately and then used together in calculation.\n",
    "    #Might have to adjust cloud cover percentage for certain cities\n",
    "    #Downloading data\n",
    "    landsat_8 = ee.ImageCollection('LANDSAT/LC08/C02/T1').filterDate(year+'-01-01', year+'-12-31').filterBounds(region).filterMetadata('CLOUD_COVER','less_than', 30)\n",
    "\n",
    "    #Select LST band from recording\n",
    "    red = landsat_8.select('B4')\n",
    "    nir = landsat_8.select('B5')\n",
    "    thermal = landsat_8.select('B10')\n",
    "\n",
    "    red_mean = red.mean()\n",
    "    nir_mean = nir.mean()\n",
    "    thermal_mean = thermal.mean()\n",
    "\n",
    "    #Export red to GoogleDrive\n",
    "    task = ee.batch.Export.image.toDrive(**{\n",
    "        'image': red_mean,\n",
    "        'description': 'LST_red',\n",
    "        'folder': city,\n",
    "        'scale': 30,\n",
    "        'crs': 'EPSG:' + str(espg),\n",
    "        'region': region.getInfo()['coordinates']\n",
    "        })\n",
    "\n",
    "    task.start()\n",
    "\n",
    "    #Export nir to GoogleDrive\n",
    "    task = ee.batch.Export.image.toDrive(**{\n",
    "        'image': nir_mean,\n",
    "        'description': 'LST_nir',\n",
    "        'folder': city,\n",
    "        'scale': 30,\n",
    "        'crs': 'EPSG:' + str(espg),\n",
    "        'region': region.getInfo()['coordinates']\n",
    "        })\n",
    "\n",
    "    task.start()\n",
    "\n",
    "    #Export red to GoogleDrive\n",
    "    task = ee.batch.Export.image.toDrive(**{\n",
    "        'image': thermal_mean,\n",
    "        'description': 'LST_thermal',\n",
    "        'folder': city,\n",
    "        'scale': 30,\n",
    "        'crs': 'EPSG:' + str(espg),\n",
    "        'region': region.getInfo()['coordinates']\n",
    "        })\n",
    "\n",
    "    task.start()\n",
    "\n",
    "    # # time.sleep(10)\n",
    "\n",
    "    # ##Land Cover##\n",
    "    # #Downloading Sentinal 2 data\n",
    "    # lc_2 = ee.ImageCollection('ESA/WorldCover/v100').filterDate('2020-01-01', '2020-12-31').filterBounds(region).first()\n",
    "\n",
    "    # #Select LST band from recording\n",
    "    # lc = lc_2.select('Map')\n",
    "\n",
    "    # #Export to GoogleDrive\n",
    "    # task = ee.batch.Export.image.toDrive(**{\n",
    "    #     'image': lc,\n",
    "    #     'description': 'landcover',\n",
    "    #     'folder': city,\n",
    "    #     'scale': 30,\n",
    "    #     'crs': 'EPSG:' + str(espg),\n",
    "    #     'region': region.getInfo()['coordinates']\n",
    "    #     })\n",
    "\n",
    "    # task.start()\n",
    "\n",
    "    # time.sleep(10)\n",
    "\n",
    "    # ##Pop grid##\n",
    "    # #Downloading WorldPop Global data\n",
    "    # worldpop = ee.ImageCollection('WorldPop/GP/100m/pop').filterDate('2020-01-01', '2020-12-31').filterBounds(region)\n",
    "    \n",
    "    # worldpop = worldpop.mean()\n",
    "    # worldpop_2 = worldpop.select('population')\n",
    "\n",
    "    # #Export to GoogleDrive\n",
    "    # task = ee.batch.Export.image.toDrive(**{\n",
    "    #     'image': worldpop_2,\n",
    "    #     'description': 'population',\n",
    "    #     'folder': city,\n",
    "    #     'scale': 100,\n",
    "    #     'crs': 'EPSG:' + str(espg),\n",
    "    #     'region': region.getInfo()['coordinates']\n",
    "    #     })\n",
    "\n",
    "    # task.start()\n",
    "    print('GEE done for ' + city)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create buffers\n",
    "\n",
    "In order to calculate the statistics of the various spatial variables extracted above, buffers are created around each counter location. These buffers are then saved as a geopackage file in each city folder in order to be extracted for calculating the values for the different spatial variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create buffers ###\n",
    "#Read the counter data into the program\n",
    "\n",
    "count = 0\n",
    "l = []\n",
    "\n",
    "for index, row in citynames_df.iterrows():\n",
    "    city = row['city']\n",
    "    espg = row['espg']\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    df2 = df[df['city'] == city]\n",
    "    df3 = df2[['name', 'latitude', 'longitude']]\n",
    "\n",
    "    #Convert to geopandas with point \n",
    "    points = [Point(xy) for xy in zip(df3.longitude, df3.latitude)]\n",
    "    df_1 = pd.DataFrame({'geometry': points})\n",
    "    gdf = gpd.GeoDataFrame(df_1, geometry='geometry')\n",
    "\n",
    "    # set the lat/lon CRS\n",
    "    gdf.crs = pyproj.CRS(4326)\n",
    "\n",
    "    # convert the CRS to a projected CRS\n",
    "    gdf = gdf.to_crs(pyproj.CRS(espg))\n",
    "\n",
    "    # create the 500m buffer around each point\n",
    "    buffered_geometries = [point.buffer(250) for point in gdf['geometry']]\n",
    "\n",
    "    # create a new geodataframe with the buffer geometries\n",
    "    buffer_gdf = gpd.GeoDataFrame(geometry=buffered_geometries)\n",
    "\n",
    "    # set the projected CRS for the buffer geodataframe\n",
    "    buffer_gdf.crs = pyproj.CRS(espg)\n",
    "\n",
    "    buffer_gdf.to_file(city_path + \"/buffer_gpkg/point_buffers1.gpkg\", layer='buffers', driver=\"GPKG\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting network statistics from OSMnx\n",
    "\n",
    "Part of the variables that are used in counter prediction are network statistics that are based on the OSMnx network sorrounding each of the counter locations. The below code extracts the networks around each of the counter locations and calculates the basic network statistics provided by OSMnx.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting network statistics ###\n",
    "\n",
    "df_keys = []\n",
    "df_values = []\n",
    "df_index = []\n",
    "locations = []\n",
    "key_saved = False\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    city = row['city']\n",
    "    lat = row['latitude']\n",
    "    lon = row['longitude']\n",
    "    point = (lat, lon)\n",
    "    city_path = directory + connect + city\n",
    "    \n",
    "    #Download network graph for area\n",
    "    G = ox.graph_from_point(point, dist=250, network_type=\"drive\", truncate_by_edge=True)\n",
    "    G_proj = ox.project_graph(G)\n",
    "\n",
    "    #Calculate statistics around the particular point\n",
    "    stats = ox.basic_stats(G)\n",
    "\n",
    "    #Delete street per node proportion and calculate 3/4-way intersection cound\n",
    "    del stats['streets_per_node_proportions']\n",
    "    if 3 in stats['streets_per_node_counts'].keys():    \n",
    "        in_3 = stats['streets_per_node_counts'][3]\n",
    "    else:\n",
    "        in_3 = 0\n",
    "    if 4 in stats['streets_per_node_counts'].keys():\n",
    "        in_4 = stats['streets_per_node_counts'][4]\n",
    "    else:\n",
    "        in_4 = 0\n",
    "    \n",
    "    stats['3_way_int_count'] = in_3\n",
    "    stats['4_way_int_count'] = in_4\n",
    "\n",
    "    #Extract all keys and values to make them into dataframe\n",
    "    keys = list(stats.keys())\n",
    "    values = list(stats.values())\n",
    "    locations.append(row['name'])\n",
    "\n",
    "    if key_saved == False:\n",
    "        df_keys = keys\n",
    "        key_saved = True\n",
    "\n",
    "    df_values.append(values)\n",
    "    df_index.append(index)\n",
    "        \n",
    "    (print('location', index, 'done'))\n",
    "\n",
    "df1 = pd.DataFrame(df_values, columns=df_keys)\n",
    "df1['locations'] = locations\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df_values, columns=df_keys)\n",
    "df1['locations'] = locations\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results to a csv in directory\n",
    "df1.to_csv(\"network_statistics_1.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caclculating median speed of sorrounding roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the median speed of roads sorrounding the buffer\n",
    "def convert_to_kmh(val):\n",
    "    if \"mph\" in val:\n",
    "        return float(val.split(\" \")[0].strip(\"'\")) * 1.60934\n",
    "    else:\n",
    "        return float(val.strip(\"' \"))\n",
    "\n",
    "median_speed_l = []\n",
    "\n",
    "for index, row in citynames_df.iterrows():\n",
    "    \n",
    "    city = row['city']\n",
    "    espg = row['espg']\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Load in cycling network data and buffer data\n",
    "    drive_net = gpd.read_file(city_path + \"/network_drive.gpkg\", layer='edges').to_crs(espg)\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers1.gpkg\").to_crs(espg)\n",
    "\n",
    "    for index, buffer in buffer_gdkg.iterrows():\n",
    "        clip = drive_net.clip(buffer.geometry)\n",
    "        median_speed = clip['maxspeed'].values\n",
    "\n",
    "        #Check for different conditions of extracted speed and convert to numpy median score\n",
    "        b = []\n",
    "        for i in median_speed:\n",
    "            if i != '':\n",
    "                if i[0] == '[':\n",
    "                    values = i[1:-1].split(',')\n",
    "                    b.extend([convert_to_kmh(val.strip(\" \") ) for val in values])\n",
    "                else:\n",
    "                    b.append(convert_to_kmh(i))\n",
    "        m_speed = np.median(np.array(b))\n",
    "        median_speed_l.append(m_speed)\n",
    "\n",
    "df8 = pd.DataFrame()\n",
    "df8['median_speed'] = median_speed_l\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.to_csv('median_speed.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating point variables\n",
    "\n",
    "These functions use the POI points extracted from OSMnx API to calculate the amount that each POI occurs around each counter location. This calculation loads both the buffers and OSMnx point location to calculate the variables. \n",
    "\n",
    "Dependencies:\n",
    "1. Buffers created\n",
    "2. OSMnx point data downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate point variables ###\n",
    "\n",
    "#Check for overlap between each polygon and points\n",
    "bike_point_list = []\n",
    "bus_stops_list = []\n",
    "restaurant_list = []\n",
    "dshops_list = []\n",
    "bshops_list = []\n",
    "traffic_s_list = []\n",
    "\n",
    "for index, row in citynames_df.iterrows():\n",
    "    \n",
    "    city = row['city']\n",
    "    espg = row['espg']\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Connect folder and load in points\n",
    "    bike_shops = gpd.read_file(city_path + \"/bikePOI.gpkg\").to_crs(espg)\n",
    "    bus_stops = gpd.read_file(city_path + \"/bus_stops.gpkg\").to_crs(espg)\n",
    "    restaurants = gpd.read_file(city_path + \"/restaurants.gpkg\").to_crs(espg)\n",
    "    # daily_shops = gpd.read_file(city_path + \"/daily_shops.gpkg\").to_crs(espg)\n",
    "    business_shops = gpd.read_file(city_path + \"/business_shops.gpkg\").to_crs(espg)\n",
    "    traffic_sig = gpd.read_file(city_path + \"/traffic_signals.gpkg\").to_crs(espg)\n",
    "\n",
    "    #Load in buffers\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers1.gpkg\").to_crs(espg)\n",
    "\n",
    "    for index, row in buffer_gdkg.iterrows():\n",
    "        temp = row.geometry\n",
    "        bike_point_counts = 0\n",
    "        bus_stop_counts = 0\n",
    "        restaurant_counts = 0\n",
    "        dshop_counts = 0\n",
    "        bshop_counts = 0\n",
    "        traffic_counts = 0\n",
    "\n",
    "        #overlap bike shops\n",
    "        for index, row in bike_shops.iterrows():\n",
    "            temp1 = row.geometry\n",
    "            \n",
    "            if temp.contains(temp1):\n",
    "                bike_point_counts += 1\n",
    "\n",
    "        #overlap bus stops\n",
    "        for index, row in bus_stops.iterrows():\n",
    "            temp1 = row.geometry\n",
    "            \n",
    "            if temp.contains(temp1):\n",
    "                bus_stop_counts += 1\n",
    "\n",
    "        #overlap restaurants\n",
    "        for index, row in restaurants.iterrows():\n",
    "            temp1 = row.geometry\n",
    "            \n",
    "            if temp.contains(temp1):\n",
    "                restaurant_counts += 1\n",
    "\n",
    "        #overlap daily shops\n",
    "        for index, row in daily_shops.iterrows():\n",
    "            temp1 = row.geometry\n",
    "            \n",
    "            if temp.contains(temp1):\n",
    "                dshop_counts += 1\n",
    "        \n",
    "        #overlap business shops\n",
    "        for index, row in business_shops.iterrows():\n",
    "            temp1 = row.geometry\n",
    "            \n",
    "            if temp.contains(temp1):\n",
    "                bshop_counts += 1\n",
    "        \n",
    "        #overlap traffic signals\n",
    "        for index, row in traffic_sig.iterrows():\n",
    "            temp1 = row.geometry\n",
    "            \n",
    "            if temp.contains(temp1):\n",
    "                traffic_counts += 1\n",
    "\n",
    "        bike_point_list.append(bike_point_counts)\n",
    "        bus_stops_list.append(bus_stop_counts)\n",
    "        restaurant_list.append(restaurant_counts)\n",
    "        dshops_list.append(dshop_counts)\n",
    "        bshops_list.append(bshop_counts)\n",
    "        traffic_s_list.append(traffic_counts)\n",
    "\n",
    "df5 = pd.DataFrame()\n",
    "\n",
    "df5['bike_points'] = bike_point_list\n",
    "df5['bus_stops'] = bus_stops_list\n",
    "df5['restaurants'] = restaurant_list\n",
    "df5['daily_shops'] = dshops_list\n",
    "df5['business_shops'] = bshops_list\n",
    "df5['traffic_signals'] = traffic_s_list\n",
    " \n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv\n",
    "df5.to_csv('point_data_export_4.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating distance to POI variables\n",
    "\n",
    "Using the greenspace and education POI locations, this algorithm calculates the network distance between each counter location and their closest greenspace and education POI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating distance variabels to POIs ###\n",
    "\n",
    "greenspace_dist_list = []\n",
    "bike_dist_list = []\n",
    "edu_dist_list = []\n",
    "station_dist_list = []\n",
    "\n",
    "for index, row in citynames_df.iterrows():\n",
    "    \n",
    "    city = row['city']\n",
    "    espg = row['espg']\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Creating dataframe\n",
    "    df2 = df[df['city'] == city]\n",
    "    df3 = df2[['name', 'latitude', 'longitude']].copy()\n",
    "\n",
    "    #Convert to geopandas with point \n",
    "    geometry = [Point(xy) for xy in zip(df3.longitude, df3.latitude)]\n",
    "    df4 = df3.drop(['longitude', 'latitude'], axis=1)\n",
    "    gdf = gpd.GeoDataFrame(df4, crs=\"EPSG:4326\", geometry=geometry)\n",
    "    gdf = gdf.to_crs(espg)\n",
    "\n",
    "    #Load in street network graph\n",
    "    G = ox.load_graphml(city_path + \"/network.graphml\")\n",
    "\n",
    "    #Load in greenspace polygons\n",
    "    greenspace = gpd.read_file(city_path + \"/greenspace.gpkg\").to_crs(espg)\n",
    "\n",
    "    for index, point in gdf.iterrows():\n",
    "        #Find closest polygon and get centroid\n",
    "        polygon_index = greenspace.distance(point.geometry).sort_values().index[0]\n",
    "        nearest_centroid = greenspace.loc[polygon_index].geometry.centroid\n",
    "\n",
    "        #Extract coordinates of point\n",
    "        x, y = point.geometry.coords.xy\n",
    "        x = x[0]\n",
    "        y = y[0]\n",
    "\n",
    "        #Find closest node\n",
    "        node_counter = ox.distance.nearest_nodes(G, x, y)\n",
    "        \n",
    "        #Extract coordinates of point\n",
    "        xx, yy = nearest_centroid.xy\n",
    "        xx = xx[0]\n",
    "        yy = yy[0]\n",
    "\n",
    "        #Find closest node\n",
    "        node_greenspace = ox.distance.nearest_nodes(G, xx, yy)\n",
    "\n",
    "        #Calculate shortest path\n",
    "        try:\n",
    "            nx.shortest_path_length(G, node_counter, node_greenspace)\n",
    "        \n",
    "        except:\n",
    "            s_path = 0\n",
    "        \n",
    "        else:\n",
    "            s_path = nx.shortest_path_length(G, node_counter, node_greenspace)\n",
    "        \n",
    "        greenspace_dist_list.append(s_path)\n",
    "\n",
    "    #Load in greenspace polygons\n",
    "    bikePOI = gpd.read_file(city_path + \"/bikePOI.gpkg\").to_crs(espg)\n",
    "\n",
    "    #Import points as gdf and loop through them\n",
    "    for index, point in gdf.iterrows():\n",
    "        polygon_index = bikePOI.distance(point.geometry).sort_values().index[0]\n",
    "        nearest_bike = bikePOI.loc[polygon_index].geometry.centroid\n",
    "\n",
    "        #Extract coordinates of point\n",
    "        x, y = point.geometry.coords.xy\n",
    "        x = x[0]\n",
    "        y = y[0]\n",
    "\n",
    "        #Find closest node\n",
    "        node_counter = ox.distance.nearest_nodes(G, x, y)\n",
    "        \n",
    "        #Extract coordinates of point\n",
    "        xx, yy = nearest_bike.xy\n",
    "        xx = xx[0]\n",
    "        yy = yy[0]\n",
    "        \n",
    "        #Find closest node\n",
    "        node_greenspace = ox.distance.nearest_nodes(G, xx, yy)\n",
    "\n",
    "        #Calculate shortest path\n",
    "        try:\n",
    "            nx.shortest_path_length(G, node_counter, node_greenspace)\n",
    "        \n",
    "        except:\n",
    "            s_path = 0\n",
    "        \n",
    "        else:\n",
    "            s_path = nx.shortest_path_length(G, node_counter, node_greenspace)\n",
    "\n",
    "        bike_dist_list.append(s_path)\n",
    "\n",
    "    #Load in eduPOI polygons\n",
    "    eduPOI = gpd.read_file(city_path + \"/eduPOI.gpkg\").to_crs(espg)\n",
    "\n",
    "    #Import points as gdf and loop through them\n",
    "    for index, point in gdf.iterrows():\n",
    "        polygon_index = eduPOI.distance(point.geometry).sort_values().index[0]\n",
    "        nearest_edu = eduPOI.loc[polygon_index].geometry.centroid\n",
    "\n",
    "        #Extract coordinates of point\n",
    "        x, y = point.geometry.coords.xy\n",
    "        x = x[0]\n",
    "        y = y[0]\n",
    "\n",
    "        #Find closest node\n",
    "        node_counter = ox.distance.nearest_nodes(G, x, y)\n",
    "        \n",
    "        #Extract coordinates of point\n",
    "        xx, yy = nearest_edu.xy\n",
    "        xx = xx[0]\n",
    "        yy = yy[0]\n",
    "        \n",
    "        #Find closest node\n",
    "        node_edu = ox.distance.nearest_nodes(G, xx, yy)\n",
    "\n",
    "        #Calculate shortest path\n",
    "        try:\n",
    "            nx.shortest_path_length(G, node_counter, node_edu)\n",
    "        \n",
    "        except:\n",
    "            s_path = 0\n",
    "        \n",
    "        else:\n",
    "            s_path = nx.shortest_path_length(G, node_counter, node_edu)\n",
    "\n",
    "        edu_dist_list.append(s_path)\n",
    "\n",
    "    #Load in eduPOI polygons\n",
    "    stations = gpd.read_file(city_path + \"/t_stations.gpkg\").to_crs(espg)\n",
    "\n",
    "    #Import points as gdf and loop through them\n",
    "    for index, point in gdf.iterrows():\n",
    "        try:\n",
    "            polygon_index = stations.distance(point.geometry).sort_values().index[0]\n",
    "        except:\n",
    "            s_path = 0\n",
    "            station_dist_list.append(s_path)\n",
    "            continue\n",
    "\n",
    "        nearest_station = stations.loc[polygon_index].geometry.centroid\n",
    "\n",
    "        #Extract coordinates of point\n",
    "        x, y = point.geometry.coords.xy\n",
    "        x = x[0]\n",
    "        y = y[0]\n",
    "\n",
    "        #Find closest node\n",
    "        node_counter = ox.distance.nearest_nodes(G, x, y)\n",
    "        \n",
    "        #Extract coordinates of point\n",
    "        xx, yy = nearest_station.xy\n",
    "        xx = xx[0]\n",
    "        yy = yy[0]\n",
    "        \n",
    "        #Find closest node\n",
    "        node_greenspace = ox.distance.nearest_nodes(G, xx, yy)\n",
    "\n",
    "        #Calculate shortest path\n",
    "        try:\n",
    "            nx.shortest_path_length(G, node_counter, node_greenspace)\n",
    "        \n",
    "        except:\n",
    "            s_path = 0\n",
    "        \n",
    "        else:\n",
    "            s_path = nx.shortest_path_length(G, node_counter, node_greenspace)\n",
    "\n",
    "        station_dist_list.append(s_path)\n",
    "\n",
    "    print(city + \" is done\")\n",
    "\n",
    "df6 = pd.DataFrame()\n",
    "df6['dist_to_greenspace'] = greenspace_dist_list\n",
    "df6['dist_to_bikePOI'] = bike_dist_list\n",
    "df6['dist_to_eduPOI'] = edu_dist_list\n",
    "df6['dist_to_railstation'] = station_dist_list\n",
    "\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv\n",
    "df6.to_csv('dist_to_green_bike.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the total length of cyclepath in buffer\n",
    "\n",
    "Using the downloaded cycling path network, this function calculates the total length of cycling infrastructure found in te buffer around each counter location. \n",
    "\n",
    "Dependencies:\n",
    "1. Buffers created\n",
    "2. Cycle infrastructure downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate total length of cyclepaths ###\n",
    "\n",
    "total_cycle_list = []\n",
    "\n",
    "for index, row in citynames_df.iterrows():\n",
    "    \n",
    "    city = row['city']\n",
    "    espg = row['espg']\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Load in cycling network data and buffer data\n",
    "    cycleways = gpd.read_file(city_path + \"/cycleways.gpkg\").to_crs(espg)\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(espg)\n",
    "\n",
    "    for index, buffer in buffer_gdkg.iterrows():\n",
    "        clip = cycleways.clip(buffer.geometry)\n",
    "        total_cycle = clip.length.sum()\n",
    "        \n",
    "        total_cycle_list.append(total_cycle)\n",
    "\n",
    "df7  = pd.DataFrame()\n",
    "df7['cycle_length'] = total_cycle_list\n",
    "\n",
    "df7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv\n",
    "df7.to_csv('cycle_length_2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and calculate building area\n",
    "\n",
    "This code extracts the buildings in the buffer around each counter location and calculates that total amount of area that the buildings cover within each buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract and calculate building area ###\n",
    "#TODO: Make km2 instead of m2\n",
    "build_dens_list = []\n",
    "df_copy = df.copy()\n",
    "\n",
    "#Extracting buildings and calculating building density around points\n",
    "for index, row in citynames_df.iterrows():\n",
    "    city = row['city']\n",
    "    espg = row['espg']\n",
    "    city_path = directory + connect + city\n",
    "    \n",
    "    #Load in buffers\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(espg)\n",
    "    df_cut = df_copy[df_copy['city'] == city]\n",
    "\n",
    "    y=0\n",
    "    for (index, row), (index1, row1) in zip(buffer_gdkg.iterrows(), df_cut.iterrows()):\n",
    "        lat = row1['latitude']\n",
    "        lon = row1['longitude']\n",
    "        point = (lat, lon)\n",
    "\n",
    "        #Extracting buildings to calculate building density and clip it to buffer\n",
    "        buildings = ox.geometries_from_point(point, tags={'building': True}, dist=250)\n",
    "        buildings = buildings.to_crs(espg)\n",
    "        buffer = buffer_gdkg.iloc[[y]]\n",
    "        y+=1\n",
    "        c_buildings =buildings.clip(buffer)\n",
    "\n",
    "        temp_l = []\n",
    "        \n",
    "        #calculate the building within clipped buffer\n",
    "        for index, x in c_buildings.iterrows():\n",
    "            temp_l.append(x.geometry)\n",
    "\n",
    "        b_series = gpd.GeoSeries(temp_l).area\n",
    "        b_total_area = b_series.sum()\n",
    "        b_total_area = b_total_area / 1000000\n",
    "        build_dens_list.append(b_total_area)\n",
    "\n",
    "    print(city + ' done')\n",
    "    break\n",
    "\n",
    "df8 = pd.DataFrame()\n",
    "df8['build_area'] = build_dens_list\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv\n",
    "df8.to_csv('building.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate LST and save as new TIF file\n",
    "\n",
    "This code takes the extracted Landsat 8 raster files in order to calculate a new raster file containing the Land Surface Temperature for each city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate LST layer###\n",
    "\n",
    "for index, row in citynames_df.iterrows():\n",
    "    \n",
    "    city = row['city']\n",
    "    espg = row['espg']\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    #Load in each band that was exported from GEE\n",
    "    red = rasterio.open(city_path + \"/LST_red.tif\")\n",
    "    nir = rasterio.open(city_path + \"/LST_nir.tif\")\n",
    "    thermal = rasterio.open(city_path + \"/LST_thermal.tif\")\n",
    "\n",
    "    #Read the bands to be used for calculation\n",
    "    redImage = red.read(1).astype('f4')\n",
    "    nirImage = nir.read(1).astype('f4')\n",
    "    thermalImage = thermal.read(1).astype('f4')\n",
    "\n",
    "    lst_image_single_window = single_window(thermalImage, redImage, nirImage, unit='celcius')\n",
    "    lst_image_single_window = lst_image_single_window - 273.15\n",
    "\n",
    "    #Define affine transformation\n",
    "    affine = red.transform\n",
    "    crs_red = red.crs\n",
    "\n",
    "    #Create new raster file with calculated variables\n",
    "    with rasterio.open(\n",
    "        city_path + \"/LST_calculated.tif\",\n",
    "        mode=\"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=lst_image_single_window.shape[0],\n",
    "        width=lst_image_single_window.shape[1],\n",
    "        count=1,\n",
    "        dtype=lst_image_single_window.dtype,\n",
    "        crs=crs_red,\n",
    "        transform=affine,\n",
    "                        ) as new_dataset:\n",
    "            new_dataset.write(lst_image_single_window, 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate raster statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate raster statistics ###\n",
    "\n",
    "#Define lists for each variable\n",
    "ndvi_mean_lst = []\n",
    "ndvi_std_lst = []\n",
    "dem_mean_lst = []\n",
    "dem_std_lst = []\n",
    "lst_mean_lst = []\n",
    "lst_std_lst = []\n",
    "lc_entropy_lst = []\n",
    "pop_sum_lst = []\n",
    "\n",
    "def entropy(x):\n",
    "    unique = np.unique(x, return_counts=True)\n",
    "    unique_count = len(unique[0])\n",
    "    unique_size = unique[1]\n",
    "    total_size = x.size\n",
    "    \n",
    "    total = 0.0\n",
    "\n",
    "    for y in unique_size:\n",
    "        r = (y/total_size)*np.log(y/total_size)\n",
    "        total = r + total\n",
    "\n",
    "    c_entropy = (total/np.log(unique_count))*-1\n",
    "\n",
    "    return c_entropy\n",
    "\n",
    "for index, row in citynames_df.iterrows():\n",
    "    city = row['city']\n",
    "    espg = row['espg']\n",
    "    city_path = directory + connect + city\n",
    "\n",
    "    # #Load in the buffers around the counter locations\n",
    "    # buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(espg)\n",
    "\n",
    "    # ##NDVI##\n",
    "    # #Load in population file\n",
    "    # ndvi = rasterio.open(city_path + \"/NVDI.tif\")\n",
    "    # arr = ndvi.read(1)\n",
    "    # affine = ndvi.transform\n",
    "\n",
    "    # statistics = zonal_stats(buffer_gdkg, arr, affine=affine, stats=['mean', 'std'])\n",
    "\n",
    "    # for x in statistics:\n",
    "    #     ndvi_mean = x['mean']\n",
    "    #     ndvi_std = x['std']\n",
    "\n",
    "    #     ndvi_mean_lst.append(ndvi_mean)\n",
    "    #     ndvi_std_lst.append(ndvi_std)\n",
    "\n",
    "    # ##DEM##\n",
    "    # #Load in population file\n",
    "    # dem = rasterio.open(city_path + \"/DEM.tif\")\n",
    "    # arr = dem.read(1)\n",
    "    # affine = dem.transform\n",
    "\n",
    "    # statistics = zonal_stats(buffer_gdkg, arr, affine=affine, stats=['mean', 'std'])\n",
    "\n",
    "    # for x in statistics:\n",
    "    #     dem_mean = x['mean']\n",
    "    #     dem_std = x['std']\n",
    "\n",
    "    #     dem_mean_lst.append(dem_mean)\n",
    "    #     dem_std_lst.append(dem_std)\n",
    "\n",
    "\n",
    "    ##LST##\n",
    "    #Load in lst files\n",
    "    lst = rasterio.open(city_path + \"/LST_calculated.tif\")\n",
    "    arr = lst.read(1)\n",
    "    affine = lst.transform\n",
    "\n",
    "    # #Load in buffer file\n",
    "    buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(espg)\n",
    "\n",
    "    statistics = zonal_stats(buffer_gdkg, arr, affine=affine, stats=['mean', 'std'])\n",
    "\n",
    "    for x in statistics:\n",
    "        lst_mean = x['mean']\n",
    "        lst_std = x['std']\n",
    "\n",
    "        lst_mean_lst.append(lst_mean)\n",
    "        lst_std_lst.append(lst_std)\n",
    "\n",
    "    \n",
    "    # ##Populationn density##\n",
    "    # #Load in population files\n",
    "    # pop = rasterio.open(city_path + \"/population.tif\")\n",
    "    # arr = pop.read(1)\n",
    "    # affine = pop.transform\n",
    "\n",
    "    # # #Load in buffer file\n",
    "    # buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(espg)\n",
    "\n",
    "    # statistics = zonal_stats(buffer_gdkg, arr, affine=affine, stats=['sum'])\n",
    "\n",
    "    # for x in statistics:\n",
    "    #     pop_sum = x['sum']\n",
    "\n",
    "    #     pop_sum_lst.append(pop_sum)\n",
    "\n",
    "    \n",
    "    # ##Land Cover entropy##\n",
    "    # lc = rasterio.open(city_path + \"/landcover.tif\")\n",
    "    # arr = lc.read(1)\n",
    "    # affine = lc.transform\n",
    "\n",
    "    # # #Load in buffer file\n",
    "    # buffer_gdkg = gpd.read_file(city_path + \"/buffer_gpkg/point_buffers.gpkg\").to_crs(espg)\n",
    "\n",
    "    # statistics = zonal_stats(buffer_gdkg, arr, affine=affine, stats=['majority'], add_stats={'entropy':entropy})\n",
    "\n",
    "    # for x in statistics:\n",
    "    #     lc_entropy = x['entropy']\n",
    "    #     lc_entropy_lst.append(lc_entropy)\n",
    "\n",
    "#Add all variables to dataframe\n",
    "df9 = pd.DataFrame()\n",
    "# df9['ndvi_mean'] = ndvi_mean_lst\n",
    "# df9['ndvi_std'] = ndvi_std_lst\n",
    "# df9['dem_mean'] = dem_mean_lst\n",
    "# df9['dem_std'] = dem_std_lst\n",
    "df9['lst_mean'] = lst_mean_lst\n",
    "df9['lst_std'] = lst_std_lst\n",
    "# df9['pop_sum'] = pop_sum_lst\n",
    "# df9['lc_entropy'] = lc_entropy_lst\n",
    "\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv\n",
    "df9.to_csv('raster_calculations_4.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate spatial lag variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cycling counter data into a Pandas DataFrame\n",
    "data = pd.read_csv(\"weekly_new.csv\")\n",
    "\n",
    "# Create a Block Weights spatial weight matrix based on country and city\n",
    "weights = libpysal.weights.block_weights(data[['country', 'city']].values)\n",
    "\n",
    "# Calculate the spatial lag variable using the weight matrix\n",
    "spatial_lag = np.zeros(len(data))\n",
    "for i in range(len(data)):\n",
    "    neighbors = weights.neighbors[i]\n",
    "    for neighbor in neighbors:\n",
    "        spatial_lag[i] += data.iloc[neighbor]['counts_week']\n",
    "\n",
    "# Add the spatial lag variable to the DataFrame\n",
    "df9 = pd.DataFrame()\n",
    "df9['spatial_lag'] = spatial_lag\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9.to_csv('spatial_lag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e04e9d70aa0476d320a89e4247bc687a844ab51af6dd849df37511de572e0dcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

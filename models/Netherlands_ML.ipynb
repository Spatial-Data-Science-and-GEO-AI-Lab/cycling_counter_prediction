{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winke/opt/anaconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "import matplotlib as plt\n",
    "from math import sqrt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data\n",
    "#Import csv and remove non-numerical variables\n",
    "df = pd.read_csv('weekly_new.csv')\n",
    "df = df.drop(['name', 'year', 'week', 'latitude', 'longitude', 'espg'] , axis=1)\n",
    "\n",
    "df = df[['counts_week', 'country', 'dist_to_greenspace', 'dist_to_edu', 'bike_points', 'bus_stops', 'business_shops', 'traffic_signals', 'cycle_length', 'dem_std',\n",
    "         'lst_mean', 'pop_sum', 'build_area', 'ndvi_mean', 'dist_to_bikePOI', 'dist_to_train', '3_way_int_count', 'median_speed', 'orientation_entropy']]\n",
    "\n",
    "country = df[df['country'] == 'Netherlands']\n",
    "\n",
    "#Create dependent and independent variable\n",
    "y = country.loc[:,'counts_week']\n",
    "X = country.drop(['counts_week', 'country'], axis=1)\n",
    "\n",
    "# Normalize dependent variable\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "y = pd.Series(data_scaled.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create traintestsplit for machine learning models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.343\n",
      "Model:                            OLS   Adj. R-squared:                  0.307\n",
      "Method:                 Least Squares   F-statistic:                     9.451\n",
      "Date:                Sun, 19 Feb 2023   Prob (F-statistic):           6.70e-20\n",
      "Time:                        13:58:57   Log-Likelihood:                -394.15\n",
      "No. Observations:                 326   AIC:                             824.3\n",
      "Df Residuals:                     308   BIC:                             892.5\n",
      "Df Model:                          17                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                   2.1758      0.503      4.327      0.000       1.186       3.165\n",
      "dist_to_greenspace      0.0009      0.001      0.801      0.424      -0.001       0.003\n",
      "dist_to_edu            -0.0004      0.001     -0.330      0.741      -0.003       0.002\n",
      "bike_points             0.0302      0.016      1.870      0.062      -0.002       0.062\n",
      "bus_stops              -0.0119      0.020     -0.608      0.544      -0.050       0.027\n",
      "business_shops         -0.0007      0.009     -0.073      0.942      -0.019       0.018\n",
      "traffic_signals         0.0101      0.007      1.541      0.124      -0.003       0.023\n",
      "cycle_length         5.073e-05   9.35e-05      0.543      0.588      -0.000       0.000\n",
      "dem_std                 0.0279      0.040      0.699      0.485      -0.051       0.106\n",
      "lst_mean               -0.1107      0.017     -6.596      0.000      -0.144      -0.078\n",
      "pop_sum                 0.0018      0.000      5.507      0.000       0.001       0.002\n",
      "build_area             -0.5158      4.219     -0.122      0.903      -8.818       7.787\n",
      "ndvi_mean              -2.0682      0.549     -3.769      0.000      -3.148      -0.989\n",
      "dist_to_bikePOI        -0.0014      0.001     -1.143      0.254      -0.004       0.001\n",
      "dist_to_train          -0.0008      0.001     -0.699      0.485      -0.003       0.001\n",
      "3_way_int_count        -0.0073      0.004     -1.641      0.102      -0.016       0.001\n",
      "median_speed           -0.0018      0.003     -0.601      0.548      -0.008       0.004\n",
      "orientation_entropy    -0.0078      0.102     -0.076      0.939      -0.209       0.193\n",
      "==============================================================================\n",
      "Omnibus:                      106.141   Durbin-Watson:                   0.986\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              274.252\n",
      "Skew:                           1.553   Prob(JB):                     2.80e-60\n",
      "Kurtosis:                       6.246   Cond. No.                     1.42e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.42e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#Add a constant to the data and run OLS regression\n",
    "Xc = sm.add_constant(X)\n",
    "Xc.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "model = sm.OLS(y, Xc).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.6571822712930534\n",
      "RMSE:  0.8106677934228381\n",
      "MAE:  0.550963333697427\n"
     ]
    }
   ],
   "source": [
    "# Predict target variable using the OLS model\n",
    "y_pred = model.predict(Xc)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          VIF             features\n",
      "0    2.549643   dist_to_greenspace\n",
      "1    2.828259          dist_to_edu\n",
      "2    1.456425          bike_points\n",
      "3    2.081215            bus_stops\n",
      "4    1.786621       business_shops\n",
      "5    2.597679      traffic_signals\n",
      "6    8.762052         cycle_length\n",
      "7    4.792365              dem_std\n",
      "8   26.723515             lst_mean\n",
      "9    9.460513              pop_sum\n",
      "10  12.426268           build_area\n",
      "11  13.608344            ndvi_mean\n",
      "12   2.588345      dist_to_bikePOI\n",
      "13   2.481977        dist_to_train\n",
      "14   7.843631      3_way_int_count\n",
      "15   8.126864         median_speed\n",
      "16  35.690567  orientation_entropy\n"
     ]
    }
   ],
   "source": [
    "Xnc = Xc.drop('const', axis=1)\n",
    "\n",
    "#Check multicolinearity with VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF\"] = [variance_inflation_factor(Xnc.values, i) for i in range(Xnc.shape[1])]\n",
    "vif[\"features\"] = Xnc.columns\n",
    "\n",
    "print(vif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best score:  -0.5509510083805538\n",
      "R-squared:  0.31299043782205893\n",
      "MSE:  0.892178636170716\n",
      "RMSE:  0.9445520822965328\n",
      "MAE:  0.6171192739868601\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400, 500],\n",
    "            'max_depth': [5, 10, 15, 20, 25],\n",
    "            'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# Create the random forest model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "#Create model from best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model_rf = RandomForestRegressor(**best_params)\n",
    "\n",
    "#Run the model on the test split of the data\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "#Calculate model statistics and print them\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"R-squared: \", r2)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which shap graph you would like to see using commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SHAP to explain things\n",
    "explainer = shap.Explainer(model_rf, X_train)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# shap_values.display_data = shap.datasets.adult(display=True)[0].values\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")\n",
    "\n",
    "shap.plots.bar(shap_values[1])\n",
    "\n",
    "shap.plots.scatter(shap_values[:,\"cycle_length\"], color=shap_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n",
      "Best parameters:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 1}\n",
      "Best score:  -0.5075663941804176\n",
      "R-squared:  0.3385914385753629\n",
      "MSE:  0.8589321324913797\n",
      "RMSE:  0.9267859151343312\n",
      "MAE:  0.6011437025448959\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {'n_estimators': [50, 100, 200, 250, 300],\n",
    "              'max_depth': [3, 5, 10, 15],\n",
    "              'learning_rate': [0.1, 0.3, 0.5, 1],\n",
    "              'subsample': [0.8, 1],\n",
    "              'colsample_bytree': [0.8, 1]\n",
    "              }\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(xgb_reg,\n",
    "                param_grid=param_grid,\n",
    "                cv=kf,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                verbose=True)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "#Create model from best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model_xgb = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "# Get the predictions of the model on the test data\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred = model_xgb.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared, RMSE, MSE and F1-score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"R-squared: \", r2)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "with open('/Users/winke/Documents/University/Thesis/Predicting_cycling/models/saved_models/NL_xgb.pkl', 'wb') as f:\n",
    "    pickle.dump(model_xgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importance\n",
    "importance = model_xgb.get_booster().get_score(importance_type='weight')\n",
    "importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Plot the feature importance\n",
    "df_importance = pd.DataFrame(importance, columns=['Feature', 'Importance'])\n",
    "df_importance.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris = pd.read_csv(\"/Users/winke/Documents/University/Thesis/Predicting_cycling/models/testing.csv\")\n",
    "paris = paris[['cycle_length', 'lst_mean', 'pop_sum', 'dem_mean', 'build_area', 'street_length_total',\n",
    "          'ndvi_mean', 'restaurants', 'dist_to_bikePOI', 'bike_points', 'daily_shops', 'median_speed', 'business_shops',\n",
    "           'traffic_signals', 'dist_to_edu', 'dist_to_train', 'streets_per_node_avg', 'circuity_avg', 'lc_entropy', 'bus_stops', 'dist_to_greenspace', '3_way_int_count']]\n",
    "\n",
    "# Load the saved model\n",
    "with open('/Users/winke/Documents/University/Thesis/Predicting_cycling/models/saved_models/all_xgb.pkl', 'rb') as f:\n",
    "    loaded_xgb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the XGBoost model to make predictions on the new dataset\n",
    "y_new_pred = loaded_xgb.predict(paris.values)\n",
    "y_series = pd.Series(y_new_pred)\n",
    "\n",
    "y_unscaled = scaler.inverse_transform(y_series.values.reshape(-1, 1))\n",
    "y = pd.Series(y_unscaled.ravel())\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which shap graph you would like to see using commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SHAP to explain things\n",
    "explainer = shap.Explainer(model_xgb, X_train)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# shap_values.display_data = shap.datasets.adult(display=True)[0].values\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")\n",
    "\n",
    "shap.plots.bar(shap_values[1])\n",
    "\n",
    "shap.plots.scatter(shap_values[:,\"pop_sum\"], color=shap_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1],\n",
    "              'kernel': ['linear', 'rbf']\n",
    "              }\n",
    "\n",
    "# Create the SVM model\n",
    "svm_reg = SVR()\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(svm_reg,\n",
    "                param_grid=param_grid,\n",
    "                cv=kf,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                verbose=True)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "#Create model from best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model_svm = SVR(**best_params)\n",
    "\n",
    "# Get the predictions of the model on the test data\n",
    "model_svm.fit(X_train, y_train)\n",
    "y_pred = model_svm.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared, RMSE, MSE and F1-score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"R-squared: \", r2)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which shap graph you would like to see using commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SHAP to explain things\n",
    "explainer = shap.Explainer(model_svm, X_train)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# shap_values.display_data = shap.datasets.adult(display=True)[0].values\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")\n",
    "\n",
    "shap.plots.bar(shap_values[1])\n",
    "\n",
    "shap.plots.scatter(shap_values[:,\"ndvi_mean\"], color=shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8794ba5723f3a63e895a9300187a0c279dca75a37113b6a48c61e29ffb05a139"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winke/opt/anaconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# United Kingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data\n",
    "#Import csv and remove non-numerical variables\n",
    "df = pd.read_csv('weekly_new.csv')\n",
    "df = df.drop(['name', 'year', 'week', 'latitude', 'longitude', 'espg'] , axis=1)\n",
    "\n",
    "df = df[['counts_week', 'country', 'dist_to_greenspace', 'dist_to_edu', 'bike_points', 'bus_stops', 'business_shops', 'traffic_signals', 'cycle_length', 'dem_std',\n",
    "         'lst_mean', 'pop_sum', 'build_area', 'ndvi_mean', 'dist_to_bikePOI', 'dist_to_train', '3_way_int_count', 'median_speed', 'orientation_entropy']]\n",
    "\n",
    "country = df[df['country'] == 'UK']\n",
    "\n",
    "#Create dependent and independent variable\n",
    "y = country.loc[:,'counts_week']\n",
    "X = country.drop(['counts_week', 'country'], axis=1)\n",
    "\n",
    "# # Normalize dependent variable\n",
    "# scaler = StandardScaler()\n",
    "# data_scaled = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "# y = pd.Series(data_scaled.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create traintestsplit for machine learning models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            counts_week   R-squared:                       0.507\n",
      "Model:                            OLS   Adj. R-squared:                  0.479\n",
      "Method:                 Least Squares   F-statistic:                     18.04\n",
      "Date:                Sun, 19 Feb 2023   Prob (F-statistic):           1.87e-36\n",
      "Time:                        15:55:00   Log-Likelihood:                -2963.1\n",
      "No. Observations:                 316   AIC:                             5962.\n",
      "Df Residuals:                     298   BIC:                             6030.\n",
      "Df Model:                          17                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const               -1174.6694   1509.260     -0.778      0.437   -4144.827    1795.488\n",
      "dist_to_greenspace     -7.7760      2.278     -3.413      0.001     -12.260      -3.293\n",
      "dist_to_edu             1.0469      2.351      0.445      0.656      -3.579       5.673\n",
      "bike_points           159.4181     24.774      6.435      0.000     110.664     208.172\n",
      "bus_stops             -32.5857     54.792     -0.595      0.552    -140.415      75.243\n",
      "business_shops        -36.8947     43.396     -0.850      0.396    -122.296      48.506\n",
      "traffic_signals        77.2031     38.785      1.991      0.047       0.876     153.531\n",
      "cycle_length            1.2088      0.353      3.421      0.001       0.514       1.904\n",
      "dem_std               -64.3261     76.191     -0.844      0.399    -214.267      85.615\n",
      "lst_mean               53.4409     46.741      1.143      0.254     -38.544     145.426\n",
      "pop_sum                 2.2722      0.470      4.837      0.000       1.348       3.197\n",
      "build_area           9384.9137   1.45e+04      0.649      0.517   -1.91e+04    3.79e+04\n",
      "ndvi_mean             831.4495   1757.087      0.473      0.636   -2626.422    4289.321\n",
      "dist_to_bikePOI         3.1440      2.497      1.259      0.209      -1.769       8.057\n",
      "dist_to_train           6.1090      2.304      2.651      0.008       1.574      10.644\n",
      "3_way_int_count       -47.1495     17.797     -2.649      0.008     -82.173     -12.126\n",
      "median_speed            6.7593     10.095      0.670      0.504     -13.108      26.627\n",
      "orientation_entropy    63.4731    353.522      0.180      0.858    -632.244     759.190\n",
      "==============================================================================\n",
      "Omnibus:                      130.535   Durbin-Watson:                   1.684\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1063.213\n",
      "Skew:                           1.480   Prob(JB):                    1.34e-231\n",
      "Kurtosis:                      11.485   Cond. No.                     9.67e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.67e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#Add a constant to the data and run OLS regression\n",
    "Xc = sm.add_constant(X)\n",
    "Xc.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "model = sm.OLS(y, Xc).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  8169801.675944452\n",
      "RMSE:  2858.2864929786956\n",
      "MAE:  1788.14410455887\n"
     ]
    }
   ],
   "source": [
    "# Predict target variable using the OLS model\n",
    "y_pred = model.predict(Xc)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           VIF              features\n",
      "0     3.230577              dem_mean\n",
      "1     4.025645          cycle_length\n",
      "2    16.178123              lst_mean\n",
      "3     5.957799               pop_sum\n",
      "4     4.533486               dem_std\n",
      "5    13.260465            build_area\n",
      "6    22.571950   street_length_total\n",
      "7    18.072460             ndvi_mean\n",
      "8     4.921063           restaurants\n",
      "9     3.019283       dist_to_bikePOI\n",
      "10   17.632002       3_way_int_count\n",
      "11    4.116422           bike_points\n",
      "12    4.136551           daily_shops\n",
      "13    6.998290          median_speed\n",
      "14    2.852930        business_shops\n",
      "15    3.636297       traffic_signals\n",
      "16    4.220737           dist_to_edu\n",
      "17    3.564103         dist_to_train\n",
      "18  102.803929  streets_per_node_avg\n",
      "19   95.957475          circuity_avg\n",
      "20   55.568380            lc_entropy\n",
      "21    4.419665             bus_stops\n",
      "22    3.579338    dist_to_greenspace\n"
     ]
    }
   ],
   "source": [
    "Xnc = Xc.drop('const', axis=1)\n",
    "\n",
    "#Check multicolinearity with VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF\"] = [variance_inflation_factor(Xnc.values, i) for i in range(Xnc.shape[1])]\n",
    "vif[\"features\"] = Xnc.columns\n",
    "\n",
    "print(vif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': 25, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best score:  -9757099.338217042\n",
      "R-squared:  0.44810234536342797\n",
      "MSE:  13372770.568640627\n",
      "RMSE:  3656.8798952988086\n",
      "MAE:  1423.1465625\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "            'max_depth': [5, 10, 15, 20, 25],\n",
    "            'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# Create the random forest model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "#Create model from best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model_rf = RandomForestRegressor(**best_params)\n",
    "\n",
    "#Run the model on the test split of the data\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "#Calculate model statistics and print them\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"R-squared: \", r2)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "with open('/Users/winke/Documents/University/Thesis/Predicting_cycling/models/saved_models/UK_rf_1.pkl', 'wb') as f:\n",
    "    pickle.dump(model_rf, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which shap graph you would like to see using commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SHAP to explain things\n",
    "explainer = shap.Explainer(model_rf, X_train)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# shap_values.display_data = shap.datasets.adult(display=True)[0].values\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")\n",
    "\n",
    "shap.plots.scatter(shap_values[:,\"ndvi_mean\"], color=shap_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n",
      "Best parameters:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 15, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Best score:  -0.560345219805496\n",
      "R-squared:  0.2815842822468537\n",
      "MSE:  1.0501828838001515\n",
      "RMSE:  1.0247843108674877\n",
      "MAE:  0.366150789525877\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {'n_estimators': [50, 100, 200, 250, 300, 400],\n",
    "              'max_depth': [3, 5, 10, 15],\n",
    "              'learning_rate': [0.1, 0.3, 0.5, 1],\n",
    "              'subsample': [0.8, 1],\n",
    "              'colsample_bytree': [0.8, 1]\n",
    "              }\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(xgb_reg,\n",
    "                param_grid=param_grid,\n",
    "                cv=kf,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                verbose=True)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "#Create model from best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model_xgb = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "# Get the predictions of the model on the test data\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred = model_xgb.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared, RMSE, MSE and F1-score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"R-squared: \", r2)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "with open('/Users/winke/Documents/University/Thesis/Predicting_cycling/models/saved_models/UK_xgb_1.pkl', 'wb') as f:\n",
    "    pickle.dump(model_xgb, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which shap graph you would like to see using commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SHAP to explain things\n",
    "explainer = shap.Explainer(model_xgb, X_train)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# shap_values.display_data = shap.datasets.adult(display=True)[0].values\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")\n",
    "\n",
    "shap.plots.bar(shap_values[1])\n",
    "\n",
    "shap.plots.scatter(shap_values[:,\"ndvi_mean\"], color=shap_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best parameters:  {'C': 0.1, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "Best score:  -0.6488648774259547\n",
      "R-squared:  0.39950083129216685\n",
      "MSE:  0.8778120148672429\n",
      "RMSE:  0.9369162261735267\n",
      "MAE:  0.538897257800182\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1],\n",
    "              'kernel': ['linear', 'rbf']\n",
    "              }\n",
    "\n",
    "# Create the SVM model\n",
    "svm_reg = SVR()\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(svm_reg,\n",
    "                param_grid=param_grid,\n",
    "                cv=kf,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                verbose=True)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "#Create model from best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model_svm = SVR(**best_params)\n",
    "\n",
    "# Get the predictions of the model on the test data\n",
    "model_svm.fit(X_train, y_train)\n",
    "y_pred = model_svm.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared, RMSE, MSE and F1-score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"R-squared: \", r2)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which shap graph you would like to see using commenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SHAP to explain things\n",
    "explainer = shap.Explainer(model_svm, X_train)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# shap_values.display_data = shap.datasets.adult(display=True)[0].values\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")\n",
    "\n",
    "shap.plots.bar(shap_values[1])\n",
    "\n",
    "shap.plots.scatter(shap_values[:,\"ndvi_mean\"], color=shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8794ba5723f3a63e895a9300187a0c279dca75a37113b6a48c61e29ffb05a139"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

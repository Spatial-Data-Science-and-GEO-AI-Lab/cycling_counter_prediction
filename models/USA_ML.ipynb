{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winke/opt/anaconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import statsmodels.api as sm\n",
    "from math import sqrt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# United States of America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data\n",
    "#Import csv and remove non-numerical variables\n",
    "df = pd.read_csv('weekly_new.csv')\n",
    "df = df.drop(['name', 'year', 'week', 'latitude', 'longitude', 'espg'] , axis=1)\n",
    "\n",
    "df = df[['counts_week', 'country', 'dist_to_greenspace', 'dist_to_edu', 'bike_points', 'bus_stops', 'business_shops', 'traffic_signals', 'cycle_length', 'dem_std',\n",
    "         'lst_mean', 'pop_sum', 'build_area', 'ndvi_mean', 'dist_to_bikePOI', 'dist_to_train', '3_way_int_count', 'median_speed', 'orientation_entropy']]\n",
    "\n",
    "country = df[df['country'] == 'USA']\n",
    "\n",
    "#Create dependent and independent variable\n",
    "y = country.loc[:,'counts_week']\n",
    "X = country.drop(['counts_week', 'country'], axis=1)\n",
    "\n",
    "# # Normalize dependent variable\n",
    "# scaler = StandardScaler()\n",
    "# data_scaled = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "# y = pd.Series(data_scaled.ravel())\n",
    "\n",
    "#Use natural log as normalization\n",
    "y = np.log(y + 1e-8).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create traintestsplit for machine learning models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            counts_week   R-squared:                       0.158\n",
      "Model:                            OLS   Adj. R-squared:                  0.101\n",
      "Method:                 Least Squares   F-statistic:                     2.782\n",
      "Date:                Sun, 19 Feb 2023   Prob (F-statistic):           0.000279\n",
      "Time:                        18:08:40   Log-Likelihood:                -582.32\n",
      "No. Observations:                 270   AIC:                             1201.\n",
      "Df Residuals:                     252   BIC:                             1265.\n",
      "Df Model:                          17                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                   7.4409      1.241      5.997      0.000       4.997       9.885\n",
      "dist_to_greenspace      0.0005      0.002      0.264      0.792      -0.003       0.004\n",
      "dist_to_edu             0.0002      0.002      0.125      0.901      -0.004       0.004\n",
      "bike_points            -0.0205      0.014     -1.480      0.140      -0.048       0.007\n",
      "bus_stops               0.0392      0.045      0.868      0.386      -0.050       0.128\n",
      "business_shops          0.0325      0.036      0.893      0.373      -0.039       0.104\n",
      "traffic_signals         0.0190      0.038      0.497      0.620      -0.056       0.094\n",
      "cycle_length            0.0004      0.000      1.570      0.118      -0.000       0.001\n",
      "dem_std                 0.0819      0.035      2.329      0.021       0.013       0.151\n",
      "lst_mean               -0.0547      0.039     -1.419      0.157      -0.131       0.021\n",
      "pop_sum                 0.0003      0.000      1.816      0.071   -2.71e-05       0.001\n",
      "build_area             -8.4751      7.718     -1.098      0.273     -23.675       6.725\n",
      "ndvi_mean              -3.1265      1.060     -2.950      0.003      -5.214      -1.039\n",
      "dist_to_bikePOI      3.209e-05      0.002      0.017      0.987      -0.004       0.004\n",
      "dist_to_train           0.0012      0.002      0.691      0.490      -0.002       0.005\n",
      "3_way_int_count         0.0047      0.016      0.306      0.760      -0.026       0.035\n",
      "median_speed            0.0080      0.006      1.399      0.163      -0.003       0.019\n",
      "orientation_entropy     0.0268      0.276      0.097      0.923      -0.516       0.570\n",
      "==============================================================================\n",
      "Omnibus:                      343.290   Durbin-Watson:                   1.456\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            39854.395\n",
      "Skew:                          -5.446   Prob(JB):                         0.00\n",
      "Kurtosis:                      61.515   Cond. No.                     6.83e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 6.83e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#Add a constant to the data and run OLS regression\n",
    "Xc = sm.add_constant(X)\n",
    "Xc.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "model = sm.OLS(y, Xc).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  4.373632259198429\n",
      "RMSE:  2.091323088190447\n",
      "MAE:  1.2711379187651404\n"
     ]
    }
   ],
   "source": [
    "# Predict target variable using the OLS model\n",
    "y_pred = model.predict(Xc)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          VIF             features\n",
      "0    2.967273   dist_to_greenspace\n",
      "1    3.665311          dist_to_edu\n",
      "2    1.679670          bike_points\n",
      "3    2.267165            bus_stops\n",
      "4    2.190108       business_shops\n",
      "5    5.155346      traffic_signals\n",
      "6    2.597300         cycle_length\n",
      "7    3.053129              dem_std\n",
      "8   12.688316             lst_mean\n",
      "9    2.001474              pop_sum\n",
      "10   6.080192           build_area\n",
      "11   6.064878            ndvi_mean\n",
      "12   3.318851      dist_to_bikePOI\n",
      "13   2.418218        dist_to_train\n",
      "14   4.814509      3_way_int_count\n",
      "15   4.221814         median_speed\n",
      "16  17.358568  orientation_entropy\n"
     ]
    }
   ],
   "source": [
    "#Check multicolinearity with VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns\n",
    "\n",
    "print(vif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Best score:  -4.578490730510742\n",
      "R-squared:  0.09169084490798107\n",
      "MSE:  2.8844600189871437\n",
      "RMSE:  1.6983698121984927\n",
      "MAE:  1.418873991955814\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "            'max_depth': [5, 10, 15, 20, 25],\n",
    "            'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# Create the random forest model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "#Create model from best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model_rf = RandomForestRegressor(**best_params)\n",
    "\n",
    "#Run the model on the test split of the data\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "#Calculate model statistics and print them\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"R-squared: \", r2)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "with open('/Users/winke/Documents/University/Thesis/Predicting_cycling/models/saved_models/US_rf_3.pkl', 'wb') as f:\n",
    "    pickle.dump(model_rf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SHAP to explain things\n",
    "explainer = shap.Explainer(model_rf, X_train)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# shap_values.display_data = shap.datasets.adult(display=True)[0].values\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")\n",
    "\n",
    "shap.plots.bar(shap_values[1])\n",
    "\n",
    "shap.plots.scatter(shap_values[:,\"ndvi_mean\"], color=shap_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n",
      "Best parameters:  {'colsample_bytree': 1, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1}\n",
      "Best score:  -4.4700891988144855\n",
      "R-squared:  0.051754834442212405\n",
      "MSE:  3.0112822852393117\n",
      "RMSE:  1.7353046664027938\n",
      "MAE:  1.3651795229763453\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {'n_estimators': [50, 100, 200, 250, 300, 400],\n",
    "              'max_depth': [3, 5, 10, 15],\n",
    "              'learning_rate': [0.1, 0.3, 0.5, 1],\n",
    "              'subsample': [0.8, 1],\n",
    "              'colsample_bytree': [0.8, 1]\n",
    "              }\n",
    "\n",
    "# Create the XGBoost model\n",
    "xgb_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(xgb_reg,\n",
    "                param_grid=param_grid,\n",
    "                cv=kf,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                verbose=True)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "#Create model from best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model_xgb = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "# Get the predictions of the model on the test data\n",
    "model_xgb.fit(X_train, y_train)\n",
    "y_pred = model_xgb.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared, RMSE, MSE and F1-score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"R-squared: \", r2)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "with open('/Users/winke/Documents/University/Thesis/Predicting_cycling/models/saved_models/US_xgb_3.pkl', 'wb') as f:\n",
    "    pickle.dump(model_xgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SHAP to explain things\n",
    "explainer = shap.Explainer(model_xgb, X_train)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# shap_values.display_data = shap.datasets.adult(display=True)[0].values\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")\n",
    "\n",
    "shap.plots.bar(shap_values[1])\n",
    "\n",
    "shap.plots.scatter(shap_values[:,\"ndvi_mean\"], color=shap_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1, 'scale', 'auto'],\n",
    "              'kernel': ['linear', 'rbf']\n",
    "              }\n",
    "\n",
    "# Create the SVM model\n",
    "svm_reg = SVR()\n",
    "\n",
    "# Create the K-fold cross-validation object\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(svm_reg,\n",
    "                param_grid=param_grid,\n",
    "                cv=kf,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                verbose=True)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "#Create model from best parameters\n",
    "best_params = grid_search.best_params_\n",
    "model_svm = SVR(**best_params)\n",
    "\n",
    "# Get the predictions of the model on the test data\n",
    "model_svm.fit(X_train, y_train)\n",
    "y_pred = model_svm.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared, RMSE, MSE and F1-score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"R-squared: \", r2)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SHAP to explain things\n",
    "explainer = shap.Explainer(model_svm, X_train)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# shap_values.display_data = shap.datasets.adult(display=True)[0].values\n",
    "shap.plots.bar(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n",
    "\n",
    "shap.plots.beeswarm(shap_values.abs, color=\"shap_red\")\n",
    "\n",
    "shap.plots.bar(shap_values[1])\n",
    "\n",
    "shap.plots.scatter(shap_values[:,\"ndvi_mean\"], color=shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8794ba5723f3a63e895a9300187a0c279dca75a37113b6a48c61e29ffb05a139"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
